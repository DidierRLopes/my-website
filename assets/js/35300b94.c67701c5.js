"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[99767],{41622:(e,n,t)=>{t.d(n,{A:()=>f});var a=t(96540),l=t(18215),s=t(15066),o=t(63427),i=t(92303),r=t(41422);const d={details:"details_lb9f",isBrowser:"isBrowser_bmU9",collapsibleContent:"collapsibleContent_i85q"};var c=t(74848);function h(e){return!!e&&("SUMMARY"===e.tagName||h(e.parentElement))}function u(e,n){return!!e&&(e===n||u(e.parentElement,n))}function g({summary:e,children:n,...t}){(0,o.A)().collectAnchor(t.id);const l=(0,i.A)(),g=(0,a.useRef)(null),{collapsed:m,setCollapsed:p}=(0,r.u)({initialState:!t.open}),[f,b]=(0,a.useState)(t.open),j=a.isValidElement(e)?e:(0,c.jsx)("summary",{children:e??"Details"});return(0,c.jsxs)("details",{...t,ref:g,open:f,"data-collapsed":m,className:(0,s.A)(d.details,l&&d.isBrowser,t.className),onMouseDown:e=>{h(e.target)&&e.detail>1&&e.preventDefault()},onClick:e=>{e.stopPropagation();const n=e.target;h(n)&&u(n,g.current)&&(e.preventDefault(),m?(p(!1),b(!0)):p(!0))},children:[j,(0,c.jsx)(r.N,{lazy:!1,collapsed:m,onCollapseTransitionEnd:e=>{p(e),b(!e)},children:(0,c.jsx)("div",{className:d.collapsibleContent,children:n})})]})}const m={details:"details_b_Ee"},p="alert alert--info";function f({...e}){return(0,c.jsx)(g,{...e,className:(0,l.A)(p,m.details,e.className)})}},55611:e=>{e.exports=JSON.parse('{"permalink":"/blog/turn-my-blog-feed-into-a-qa-dataset-to-fine-tune-a-llm","editUrl":"https://github.com/DidierRLopes/my-website/tree/main/blog/2025-01-21-turn-my-blog-feed-into-a-qa-dataset-to-fine-tune-a-llm.md","source":"@site/blog/2025-01-21-turn-my-blog-feed-into-a-qa-dataset-to-fine-tune-a-llm.md","title":"Turn my blog feed into a QA dataset to fine-tune a LLM","description":"This project converts blog feed content into a structured Question-Answer dataset using LLaMA 3.2 (via Ollama) for local processing. The generated dataset follows a conversational format and can be automatically pushed to Hugging Face.","date":"2025-01-21T00:00:00.000Z","tags":[{"inline":true,"label":"ai","permalink":"/blog/tags/ai"},{"inline":true,"label":"ml","permalink":"/blog/tags/ml"},{"inline":true,"label":"llm","permalink":"/blog/tags/llm"},{"inline":true,"label":"dataset","permalink":"/blog/tags/dataset"},{"inline":true,"label":"hugging-face","permalink":"/blog/tags/hugging-face"},{"inline":true,"label":"ollama","permalink":"/blog/tags/ollama"},{"inline":true,"label":"llama","permalink":"/blog/tags/llama"},{"inline":true,"label":"fine-tuning","permalink":"/blog/tags/fine-tuning"},{"inline":true,"label":"python","permalink":"/blog/tags/python"}],"readingTime":5.05,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"turn-my-blog-feed-into-a-qa-dataset-to-fine-tune-a-llm","title":"Turn my blog feed into a QA dataset to fine-tune a LLM","date":"2025-01-21T00:00:00.000Z","image":"/blog/2025-01-21-turn-my-blog-feed-into-a-qa-dataset-to-fine-tune-a-llm","tags":["ai","ml","llm","dataset","hugging-face","ollama","llama","fine-tuning","python"],"description":"This project converts blog feed content into a structured Question-Answer dataset using LLaMA 3.2 (via Ollama) for local processing. The generated dataset follows a conversational format and can be automatically pushed to Hugging Face.","hideSidebar":true},"unlisted":false,"prevItem":{"title":"Building a developer friendly interface for financial analysts","permalink":"/blog/building-a-developer-friendly-interface-for-financial-analysts"},"nextItem":{"title":"There\'s a zero percent chance that open source doesn\'t win","permalink":"/blog/theres-a-zero-percent-chance-that-open-source-doesnt-win"}}')},69221:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>d,default:()=>g,frontMatter:()=>r,metadata:()=>a,toc:()=>h});var a=t(55611),l=t(74848),s=t(28453),o=t(73748),i=t(41622);const r={slug:"turn-my-blog-feed-into-a-qa-dataset-to-fine-tune-a-llm",title:"Turn my blog feed into a QA dataset to fine-tune a LLM",date:new Date("2025-01-21T00:00:00.000Z"),image:"/blog/2025-01-21-turn-my-blog-feed-into-a-qa-dataset-to-fine-tune-a-llm",tags:["ai","ml","llm","dataset","hugging-face","ollama","llama","fine-tuning","python"],description:"This project converts blog feed content into a structured Question-Answer dataset using LLaMA 3.2 (via Ollama) for local processing. The generated dataset follows a conversational format and can be automatically pushed to Hugging Face.",hideSidebar:!0},d=void 0,c={authorsImageUrls:[]},h=[{value:"Getting Started",id:"getting-started",level:2},{value:"1. Install dependencies",id:"1-install-dependencies",level:4},{value:"2. Install Ollama and pull Llama 3.2",id:"2-install-ollama-and-pull-llama-32",level:4},{value:"3. Configure Hugging Face",id:"3-configure-hugging-face",level:4},{value:"Usage",id:"usage",level:2},{value:"Dataset Format",id:"dataset-format",level:2},{value:"Summary of how it works",id:"summary-of-how-it-works",level:2}];function u(e){const n={a:"a",code:"code",h2:"h2",h4:"h4",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)("p",{align:"center",children:(0,l.jsx)("img",{width:"900",src:"/blog/2025-01-21-turn-my-blog-feed-into-a-qa-dataset-to-fine-tune-a-llm.png"})}),"\n",(0,l.jsx)(n.p,{children:"This project converts blog feed content into a structured Question-Answer dataset using LLaMA 3.2 (via Ollama) for local processing. The generated dataset follows a conversational format and can be automatically pushed to Hugging Face."}),"\n",(0,l.jsxs)(n.p,{children:["The open source code is available ",(0,l.jsx)(n.a,{href:"https://github.com/DidierRLopes/turn-blog-feed-into-qa-dataset",children:"here"}),"."]}),"\n","\n",(0,l.jsx)("div",{style:{borderTop:"1px solid #0088CC",margin:"1.5em 0"}}),"\n",(0,l.jsx)(n.p,{children:'I was looking to fine-tune an open source LLM with content that I have produced in the past to see how advanced such LLMs were and how close I could get a model running locally to "output tokens" the same way I would.'}),"\n",(0,l.jsxs)(n.p,{children:["According to Daniel Kahneman and his book ",(0,l.jsx)("a",{href:"https://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374533555",target:"_blank",rel:"noopener noreferrer",children:"Thinking, Fast and Slow"}),", humans have two modes of thought:"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"System 1"}),": Fast, instinctive and emotional. An example of this are my ",(0,l.jsx)("a",{href:"https://x.com/didier_lopes",target:"_blank",rel:"noopener noreferrer",children:"posts on X"}),"."]}),"\n"]}),"\n",(0,l.jsxs)(n.p,{children:["There are multiple libraries out there to scrape data from X. One that I used recently, and liked (without requiring an X API key) was ",(0,l.jsx)("a",{href:"https://github.com/elizaOS/twitter-scraper-finetune",target:"_blank",rel:"noopener noreferrer",children:"Twitter scraper finetune from ElizaOS"}),"."]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"System 2"}),": Slower, more deliberative and more logic. An example of this is my blog, where some of these posts take me several hours to write and need to sleep on the topic before pushing."]}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:"For this, I didn't find any good out-of-the-box library that allowed me to convert my posts into a QA dataset to fine-tune a model."}),"\n",(0,l.jsx)(n.p,{children:"So this is what I ended up building."}),"\n",(0,l.jsx)(n.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,l.jsx)(n.p,{children:"In order to do this you will need:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Python 3.11"}),"\n",(0,l.jsx)(n.li,{children:"Poetry (for python dependencies)"}),"\n",(0,l.jsx)(n.li,{children:"Ollama (to run Llama 3.2)"}),"\n",(0,l.jsx)(n.li,{children:"Hugging Face account (for dataset upload)"}),"\n"]}),"\n",(0,l.jsxs)(n.p,{children:["and obviously your blog in a JSON feed like ",(0,l.jsx)("a",{href:"https://didierlopes.com/blog/feed.json",target:"_blank",rel:"noopener noreferrer",children:(0,l.jsx)(n.a,{href:"https://didierlopes.com/blog/feed.json",children:"https://didierlopes.com/blog/feed.json"})}),"."]}),"\n",(0,l.jsx)(n.h4,{id:"1-install-dependencies",children:"1. Install dependencies"}),"\n",(0,l.jsx)(o.A,{language:"bash",children:"poetry install\npoetry run python -m spacy download en_core_web_sm"}),"\n",(0,l.jsx)(n.h4,{id:"2-install-ollama-and-pull-llama-32",children:"2. Install Ollama and pull Llama 3.2"}),"\n",(0,l.jsxs)(n.p,{children:["Follow instructions to install Ollama: ",(0,l.jsx)(n.a,{href:"https://ollama.com/",children:"https://ollama.com/"})]}),"\n",(0,l.jsxs)(n.p,{children:["Select a model to run locally using ",(0,l.jsx)(n.a,{href:"https://ollama.com/search",children:"https://ollama.com/search"}),"."]}),"\n",(0,l.jsxs)(n.p,{children:["In this case, we want to run ",(0,l.jsx)(n.code,{children:"llama3.2:latest"})," (",(0,l.jsx)(n.a,{href:"https://ollama.com/library/llama3.2",children:"https://ollama.com/library/llama3.2"}),")."]}),"\n",(0,l.jsx)(o.A,{language:"bash",children:"ollama pull llama3.2:latest"}),"\n",(0,l.jsx)("p",{align:"center",children:(0,l.jsx)("img",{width:"900",src:"/blog/2025-01-21-turn-my-blog-feed-into-a-qa-dataset-to-fine-tune-a-llm_1.png"})}),"\n",(0,l.jsx)(n.p,{children:"Then, we can check that the model has been downloaded with:"}),"\n",(0,l.jsx)(o.A,{language:"bash",children:"ollama list"}),"\n",(0,l.jsx)("p",{align:"center",children:(0,l.jsx)("img",{width:"900",src:"/blog/2025-01-21-turn-my-blog-feed-into-a-qa-dataset-to-fine-tune-a-llm_2.png"})}),"\n",(0,l.jsx)(n.p,{children:"Finally, we can test that it works with:"}),"\n",(0,l.jsx)(o.A,{language:"bash",children:"ollama run llama3.2:latest"}),"\n",(0,l.jsx)("p",{align:"center",children:(0,l.jsx)("img",{width:"900",src:"/blog/2025-01-21-turn-my-blog-feed-into-a-qa-dataset-to-fine-tune-a-llm_3.png"})}),"\n",(0,l.jsx)(n.h4,{id:"3-configure-hugging-face",children:"3. Configure Hugging Face"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:["Create a write-enabled token at ",(0,l.jsx)(n.a,{href:"https://huggingface.co/docs/hub/en/security-tokens",children:"Hugging Face"})]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:["Create a ",(0,l.jsx)(n.code,{children:".env"})," file:"]}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(o.A,{language:"bash",children:"HF_TOKEN=your_token_here"}),"\n",(0,l.jsx)(n.h2,{id:"usage",children:"Usage"}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsxs)(n.strong,{children:["1. Update the blog feed URL in ",(0,l.jsx)("a",{href:"https://github.com/DidierRLopes/turn-blog-feed-into-qa-dataset/blob/main/turn-blog-feed-into-qa-dataset.ipynb",target:"_blank",rel:"noopener noreferrer",children:"this notebook"}),"."]})}),"\n",(0,l.jsxs)(n.p,{children:["Below you can see the feed structure being used - which is the default coming from ",(0,l.jsx)("a",{href:"https://docusaurus.io/docs/blog",target:"_blank",rel:"noopener noreferrer",children:"Docusaurus"}),", which is the framework I'm using to auto-generate the feed for my personal blog."]}),"\n",(0,l.jsx)(o.A,{language:"python",children:'url = "https://didierlopes.com/blog/feed.json"'}),"\n",(0,l.jsx)(i.A,{summary:"JSON Feed Structure",children:(0,l.jsx)(o.A,{language:"json",children:'{\n"version": "https://jsonfeed.org/version/1",\n"title": "Didier Lopes Blog", \n"home_page_url": "https://didierlopes.com/blog",\n"description": "Didier Lopes Blog",\n"items": [\n  {\n    "id": "URL of the post",\n    "content_html": "HTML content of the post", \n    "url": "URL of the post",\n    "title": "Title of the post",\n    "summary": "Brief summary of the post",\n    "date_modified": "ISO 8601 date format",\n    "tags": [\n      "array",\n      "of", \n      "tags"\n    ]\n  },\n  // ... more items\n]\n}'})}),"\n",(0,l.jsx)("br",{}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"2. Set your Hugging Face dataset repository name:"})}),"\n",(0,l.jsx)(o.A,{language:"python",children:'dataset_repo = "didierlopes/my-blog-qa-dataset"'}),"\n",(0,l.jsxs)(n.p,{children:["This is what the dataset will look like in HuggingFace: ",(0,l.jsx)(n.a,{href:"https://huggingface.co/datasets/didierlopes/my-blog-qa-dataset/viewer",children:"https://huggingface.co/datasets/didierlopes/my-blog-qa-dataset/viewer"}),"."]}),"\n",(0,l.jsx)("p",{align:"center",children:(0,l.jsx)("img",{width:"900",src:"/blog/2025-01-21-turn-my-blog-feed-into-a-qa-dataset-to-fine-tune-a-llm_4.png"})}),"\n",(0,l.jsx)("br",{}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"3. Run the notebook cells sequentially."})}),"\n",(0,l.jsx)(n.p,{children:"The notebook contains detailed explanations throughout to guide you through the process step-by-step."}),"\n",(0,l.jsx)(n.h2,{id:"dataset-format",children:"Dataset Format"}),"\n",(0,l.jsx)(n.p,{children:"The generated dataset includes:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"title"}),": Blog post title"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"conversation"}),": Array of Q&A pairs in role-based format"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"context"}),": Original cleaned blog content"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"url"}),": Source blog post URL"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"date"}),": Publication date"]}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:"Note: This is the format of the conversation field:"}),"\n",(0,l.jsx)(o.A,{language:"python",children:'conversation = [\n  {\n      "role": "user", \n      "content": (\n          "You mentioned that when ChatGPT launched, everyone rushed to build "\n          "financial chatbots. What were some of the fundamental truths that "\n          "those who built these chatbots missed?"\n      )\n  },\n  {\n      "role": "assistant",\n      "content": (\n          "Those building financial chatbots missed two fundamental truths:"\n          "1. AI models are useless without access to your data."\n          "2. Access to data isn\'t enough - AI needs to handle complete "\n          "workflows, not just conversations."\n          "These limitations led to chatbots that can\'t access proprietary "\n          "data, can\'t handle complex workflows and restrict analysts to an"\n          "unnatural chat interface."\n      )\n  },\n  # ... more Q&A pairs following the same pattern\n]'}),"\n",(0,l.jsx)(n.h2,{id:"summary-of-how-it-works",children:"Summary of how it works"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsx)(n.li,{children:"Fetches blog content from JSON feed"}),"\n",(0,l.jsx)(n.li,{children:"Cleans HTML to markdown format"}),"\n",(0,l.jsx)(n.li,{children:"Analyzes sentence count to determine Q&A pair quantity"}),"\n",(0,l.jsx)(n.li,{children:"Generates contextual questions using LLaMA 3.2 running locally"}),"\n",(0,l.jsx)(n.li,{children:"Creates corresponding answers"}),"\n",(0,l.jsx)(n.li,{children:"Filters and removes duplicate Q&A pairs"}),"\n",(0,l.jsx)(n.li,{children:"Formats data for Hugging Face"}),"\n",(0,l.jsx)(n.li,{children:"Pushes to Hugging Face Hub"}),"\n"]})]})}function g(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(u,{...e})}):u(e)}}}]);