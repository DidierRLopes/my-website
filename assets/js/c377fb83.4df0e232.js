"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[20900],{41622:(e,n,i)=>{i.d(n,{A:()=>f});var t=i(96540),s=i(18215),a=i(15066),r=i(63427),o=i(92303),l=i(41422);const d={details:"details_lb9f",isBrowser:"isBrowser_bmU9",collapsibleContent:"collapsibleContent_i85q"};var c=i(74848);function h(e){return!!e&&("SUMMARY"===e.tagName||h(e.parentElement))}function p(e,n){return!!e&&(e===n||p(e.parentElement,n))}function m({summary:e,children:n,...i}){(0,r.A)().collectAnchor(i.id);const s=(0,o.A)(),m=(0,t.useRef)(null),{collapsed:u,setCollapsed:g}=(0,l.u)({initialState:!i.open}),[f,x]=(0,t.useState)(i.open),j=t.isValidElement(e)?e:(0,c.jsx)("summary",{children:e??"Details"});return(0,c.jsxs)("details",{...i,ref:m,open:f,"data-collapsed":u,className:(0,a.A)(d.details,s&&d.isBrowser,i.className),onMouseDown:e=>{h(e.target)&&e.detail>1&&e.preventDefault()},onClick:e=>{e.stopPropagation();const n=e.target;h(n)&&p(n,m.current)&&(e.preventDefault(),u?(g(!1),x(!0)):g(!0))},children:[j,(0,c.jsx)(l.N,{lazy:!1,collapsed:u,onCollapseTransitionEnd:e=>{g(e),x(!e)},children:(0,c.jsx)("div",{className:d.collapsibleContent,children:n})})]})}const u={details:"details_b_Ee"},g="alert alert--info";function f({...e}){return(0,c.jsx)(m,{...e,className:(0,s.A)(g,u.details,e.className)})}},74609:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>d,default:()=>m,frontMatter:()=>l,metadata:()=>t,toc:()=>h});var t=i(88897),s=i(74848),a=i(28453),r=i(73748),o=i(41622);const l={slug:"fine-tuning-a-llm-on-my-blog-posts",title:"Fine-tuning a LLM on my blog posts",date:new Date("2025-09-02T00:00:00.000Z"),image:"/blog/2025-09-02-fine-tuning-a-llm-on-my-blog-posts",tags:["ai","machine-learning","open-source","tutorial","apple-silicon","llm","fine-tuning"],description:"Ever wondered what it would be like to have an AI that writes exactly in your style? I did. And in this post, I share what I did about it. This is a very practical guide on how to fine-tune an LLM using LoRA with MLX on Apple Silicon.",hideSidebar:!0},d=void 0,c={authorsImageUrls:[]},h=[{value:"Context",id:"context",level:2},{value:"0. Setting up the foundation",id:"0-setting-up-the-foundation",level:2},{value:"Model",id:"model",level:3},{value:"Finetuning Technique",id:"finetuning-technique",level:3},{value:"Framework",id:"framework",level:3},{value:"1. Preparing the data",id:"1-preparing-the-data",level:2},{value:"Formatting for phi-3-mini-4k-instruct",id:"formatting-for-phi-3-mini-4k-instruct",level:3},{value:"Training split",id:"training-split",level:3},{value:"2. Train the model",id:"2-train-the-model",level:2},{value:"Model configuration",id:"model-configuration",level:3},{value:"Lora layers",id:"lora-layers",level:4},{value:"Attention matrices",id:"attention-matrices",level:4},{value:"Rank",id:"rank",level:4},{value:"Scale",id:"scale",level:4},{value:"Dropout",id:"dropout",level:4},{value:"Training Configuration",id:"training-configuration",level:3},{value:"The training process",id:"the-training-process",level:3},{value:"3. Evaluate the model",id:"3-evaluate-the-model",level:2},{value:"Step 1: Loading models for comparison",id:"step-1-loading-models-for-comparison",level:3},{value:"Step 2: Test data generation",id:"step-2-test-data-generation",level:3},{value:"Step 3: Measuring performance",id:"step-3-measuring-performance",level:3},{value:"Results and statistical analysis",id:"results-and-statistical-analysis",level:2},{value:"Performance improvement",id:"performance-improvement",level:3},{value:"Consistency analysis",id:"consistency-analysis",level:3},{value:"Range expansion",id:"range-expansion",level:3},{value:"Length analysis",id:"length-analysis",level:3},{value:"Parameter efficiency",id:"parameter-efficiency",level:3},{value:"Statistical interpretation",id:"statistical-interpretation",level:3},{value:"Vibe checking",id:"vibe-checking",level:2},{value:"Model on Hugging Face",id:"model-on-hugging-face",level:2},{value:"Fused model",id:"fused-model",level:2},{value:"Wrap up",id:"wrap-up",level:2}];function p(e){const n={a:"a",blockquote:"blockquote",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)("p",{align:"center",children:(0,s.jsx)("img",{width:"600",src:"/blog/\n2025-09-02-fine-tuning-a-llm-on-my-blog-posts.png"})}),"\n",(0,s.jsx)(n.p,{children:"Ever wondered what it would be like to have an AI that writes exactly in your style? I did. And in this post, I share what I did about it. This is a very practical guide on how to fine-tune an LLM using LoRA with MLX on Apple Silicon."}),"\n","\n",(0,s.jsx)("div",{style:{borderTop:"1px solid #0088CC",margin:"1.5em 0"}}),"\n",(0,s.jsxs)(n.p,{children:["At the start of the year I shared ",(0,s.jsx)(n.a,{href:"https://didierlopes.com/blog/turn-my-blog-feed-into-a-qa-dataset-to-fine-tune-a-llm",children:"this blogpost"}),", which converted all my blogs into a Q&A dataset that I could use to fine-tune a LLM."]}),"\n",(0,s.jsx)("p",{align:"center",children:(0,s.jsx)("img",{width:"600",src:"/blog/\n2025-09-02-fine-tuning-a-llm-on-my-blog-posts_1.png"})}),"\n",(0,s.jsxs)(n.p,{children:["After sharing this, I spent time trying to fine-tune an LLM - but results were not great and combined with all things happening at OpenBB I didn't have time to dedicate a lot of time to this. ",":upside-down-hf-logo"]}),"\n",(0,s.jsx)(n.p,{children:"But I hate leaving things half way. And this task didn\u2019t leave my TODO for the past 6 months."}),"\n",(0,s.jsx)(n.p,{children:"So I finally took things to my hands last weekend, and I\u2019m going to share the entire journey on what, why and how."}),"\n",(0,s.jsxs)(n.p,{children:["Buckle up, this will be a long post - and more technical than previous ones. And all the code will be available here: ",(0,s.jsx)(n.a,{href:"https://github.com/DidierRLopes/fine-tune-llm",children:"https://github.com/DidierRLopes/fine-tune-llm"}),"."]}),"\n",(0,s.jsx)(n.h2,{id:"context",children:"Context"}),"\n",(0,s.jsx)(n.p,{children:"Most AI models are like Wikipedia - they know a little about everything but lack the depth and personality that comes from lived experience."}),"\n",(0,s.jsx)(n.p,{children:"Think of it this way: RAG is like giving someone a reference book during an exam. Fine-tuning is like actually teaching them the subject until it becomes part of how they think."}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.em,{children:"\u201cOnce you\u2019ve maximized the performance gains from prompting, you might wonder whether to do RAG or finetuning next. The answer depends on whether your model\u2019s failures are information-based or behavior-based."})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.em,{children:"If the model fails because it lacks information, a RAG syustem that gives the model access to the relevant sources of information can help. (\u2026) On the other hand, if the model has behavioral issues, finetuning might help.\u201d"})}),"\n",(0,s.jsx)("br",{}),"\n",(0,s.jsx)(n.p,{children:"- Chip Huyen\u2019s - AI Engineering (Chapter 7: Finetuning)"}),"\n"]}),"\n",(0,s.jsx)("br",{}),"\n",(0,s.jsx)(n.p,{children:"When you fine-tune a model on your writing, you're not just feeding it information (particularly with small models and a LoRA - you're rewiring how it processes and responds to ideas. The same neural pathways that learned to write about quantum physics now learn your specific way of sharing thoughts on open source, MCP, boxing, and others."}),"\n",(0,s.jsx)(n.p,{children:'In this case, because we will fine-tune an instruct model - even the system prompt becomes part of this personalization process from the very first token. It\u2019s not a simple \u201cYou are a helpful assistant" but \u201cYou are Didier, CEO of OpenBB. You write with clarity and impact, focusing on fintech, open source, AI, and the future of research workflows\u201d.'}),"\n",(0,s.jsx)(n.p,{children:"This will result in a fine-tuned model that thinks in your voice and operates with your expertise baseline. To some extent that is, we will see later that the information transfer could be better. I attribute that to the fact that we are using a small (3.8B model), we are doing partial fine-tuning (only 0.08% of weights will be updated) and I didn\u2019t spend a lot of time iterating on the hyperparameters."}),"\n",(0,s.jsx)(n.h2,{id:"0-setting-up-the-foundation",children:"0. Setting up the foundation"}),"\n",(0,s.jsx)(n.h3,{id:"model",children:"Model"}),"\n",(0,s.jsxs)(n.p,{children:["I chose ",(0,s.jsx)(n.strong,{children:"Microsoft's Phi-3 mini model (3.8B parameters)"}),' for several strategic reasons beyond just "it fits on my Mac":']}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Technical sweet spot"}),": At 3.8B parameters, Phi-3 mini hits the perfect balance - large enough to produce coherent, contextual responses, but small enough to fine-tune efficiently on consumer hardware. Larger models like 7B+ would require more aggressive quantization."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Instruct-optimized foundation"}),": This isn't a raw base model. Phi-3 mini is already instruction-tuned with supervised fine-tuning (SFT) and likely RLHF, meaning it understands how to follow prompts and maintain conversational flow. This gives me a much better starting point than training from a base model. Note: Microsoft actually did not release the base model."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Ecosystem support"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["This ",(0,s.jsx)(n.a,{href:"https://gist.github.com/andrewssobral/89ca0cd40e609a32c0ce8241d01f484d",children:"code reference"})," gave me a working starting point"]}),"\n",(0,s.jsxs)(n.li,{children:["There was an ",(0,s.jsx)(n.a,{href:"https://github.com/microsoft/PhiCookBook/blob/main/md/03.FineTuning/FineTuning_MLX.md",children:"official cookbook"})," with best practices"]}),"\n",(0,s.jsxs)(n.li,{children:["There was a good ",(0,s.jsx)(n.a,{href:"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct",children:"model card on Hugging Face"})," with clear usage example"]}),"\n"]}),"\n",(0,s.jsx)("br",{}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Hardware compatibility"}),": With my M3 Max and 48GB RAM, this model fits comfortably in memory with room for LoRA adapters and training overhead."]}),"\n",(0,s.jsx)(n.h3,{id:"finetuning-technique",children:"Finetuning Technique"}),"\n",(0,s.jsx)(n.p,{children:"Traditional fine-tuning updates all 3.8 billion parameters, requiring enormous compute resources and risking catastrophic forgetting (where the model loses its general capabilities while learning your specific data)."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"LoRA's elegant solution"}),": Low-Rank Adaptation works by decomposing weight updates into smaller matrices. Instead of modifying a large weight matrix W directly, LoRA adds two smaller matrices ",(0,s.jsx)(n.code,{children:"A"})," and ",(0,s.jsx)(n.code,{children:"B"})," such that the update becomes ",(0,s.jsx)(n.code,{children:"W + BA"}),", where ",(0,s.jsx)(n.code,{children:"B"})," has rank ",(0,s.jsx)(n.code,{children:"r"})," and ",(0,s.jsx)(n.code,{children:"r << d"})," (with ",(0,s.jsx)(n.code,{children:"d"})," being the original dimensions). More on LoRA ",(0,s.jsx)(n.a,{href:"https://huggingface.co/docs/peft/developer_guides/lora",children:"here"}),"."]}),"\n",(0,s.jsx)("p",{align:"center",children:(0,s.jsx)("img",{width:"600",src:"/blog/\n2025-09-02-fine-tuning-a-llm-on-my-blog-posts_2.png"})}),"\n",(0,s.jsx)(n.p,{children:"Why this matters:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Parameter efficiency"}),": I'm only training a small percentage (",(0,s.jsx)(n.code,{children:"<0.2%"}),") of the entire 3.8b model"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Memory efficiency"}),": Base model stays frozen, only adapter weights need gradients"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Modularity"}),": Can swap different LoRA adapters for different tasks/personalities"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reduced overfitting"}),": Smaller parameter space makes it harder to memorize training data (which also validates the fact that fine-tuning is not best choice to give more information to a model)"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"framework",children:"Framework"}),"\n",(0,s.jsx)(n.p,{children:"MLX is specifically designed for Apple's unified memory architecture. While PyTorch can run on Mac, it wasn't built with Apple Silicon's unique characteristics in mind."}),"\n",(0,s.jsx)(n.p,{children:"Key MLX benefits:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Memory efficiency"}),": Unified memory means no CPU/GPU transfers, LoRA adapters and base model share the same memory pool efficiently"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Lazy evaluation"}),": Only computes what's needed, when it's needed - crucial for memory-constrained fine-tuning"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Native optimization"}),": Built for Apple's AMX (Apple Matrix Extensions) and Neural Engine integration"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Most production fine-tuning still happens on NVIDIA GPUs with PyTorch. But for Apple Silicon users, MLX offers several advantages:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Lower barrier to entry"}),": No need for cloud GPUs or expensive NVIDIA hardware"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Rapid experimentation"}),": Faster iteration cycles for smaller models"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Privacy"}),": Everything runs locally, no data leaves your machine"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Note that I was able to do this because I was working with a <10B parameter model and had Apple Silicon with 48GB RAM. But more importantly, this was done for experimentation, and not production - so I chose what allowed me to get my hands dirty faster."}),"\n",(0,s.jsx)(n.h2,{id:"1-preparing-the-data",children:"1. Preparing the data"}),"\n",(0,s.jsxs)(n.p,{children:["Code can be found here: ",(0,s.jsx)(n.a,{href:"https://github.com/DidierRLopes/fine-tune-llm/blob/main/scripts/01_prepare_data.py",children:"https://github.com/DidierRLopes/fine-tune-llm/blob/main/scripts/01_prepare_data.py"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["For the data we will be using a ",(0,s.jsx)(n.a,{href:"https://huggingface.co/datasets/didierlopes/my-blog-qa-dataset",children:"Q&A dataset based on my blogposts"}),". The repository where I turned my blog posts into this dataset can be found ",(0,s.jsx)(n.a,{href:"https://github.com/DidierRLopes/turn-blog-feed-into-qa-dataset/tree/main",children:"here"}),"."]}),"\n",(0,s.jsx)("p",{align:"center",children:(0,s.jsx)("img",{width:"600",src:"/blog/\n2025-09-02-fine-tuning-a-llm-on-my-blog-posts_3.png"})}),"\n",(0,s.jsx)(n.p,{children:"The dataset contains 91 blog posts transformed into conversational Q&A pairs - roughly 2,100 exchanges covering everything from OpenBB's journey to technical deep-dives on open source."}),"\n",(0,s.jsxs)(n.p,{children:["Each entry in the dataset contains conversations with user questions and my responses. But raw conversational data (which I parsed from a blogpost) isn't something you can just throw at a model. It needs structure, and more importantly, ",(0,s.jsx)(n.strong,{children:"it needs the right structure"})," for your chosen model."]}),"\n",(0,s.jsx)(n.h3,{id:"formatting-for-phi-3-mini-4k-instruct",children:"Formatting for phi-3-mini-4k-instruct"}),"\n",(0,s.jsx)(n.p,{children:"Phi-3-mini-4k-instruct has been trained with a specific chat template, and we need to follow it - otherwise results won't be optimal (this was one of my first mistakes!)"}),"\n",(0,s.jsx)("p",{align:"center",children:(0,s.jsx)("img",{width:"600",src:"/blog/\n2025-09-02-fine-tuning-a-llm-on-my-blog-posts_4.png"})}),"\n",(0,s.jsxs)(n.p,{children:["You can find that template in the model card on HF: ",(0,s.jsx)(n.a,{href:"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct",children:"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct"})]}),"\n",(0,s.jsx)(n.p,{children:"Important: Since this is an instruct model, then it is important to retain the system prompt on the training samples. (I also did a mistake here!)"}),"\n",(0,s.jsx)(n.p,{children:"Example:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"<|system|>\nYou are a helpful assistant.<|end|>\n<|user|>\nHow to explain Internet for a medieval knight?<|end|>\n<|assistant|>\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Those special tokens (",(0,s.jsx)(n.code,{children:"<|system|>"}),", ",(0,s.jsx)(n.code,{children:"<|user|>"}),", ",(0,s.jsx)(n.code,{children:"<|assistant|>"}),", ",(0,s.jsx)(n.code,{children:"<|end|>"}),") aren't decorative, they're semantic markers that tell the model exactly where each part of the conversation begins and ends. (Do not forget these, and ensure there are no typos! I did not do a mistake here ehe)"]}),"\n",(0,s.jsx)(n.p,{children:"I actually added a function to validate if the required tokens existed, and are in the right order."}),"\n",(0,s.jsx)(n.h3,{id:"training-split",children:"Training split"}),"\n",(0,s.jsx)(n.p,{children:"One of the most common mistakes in fine-tuning is treating your test data as validation data. Here's how I split the ~2,100 samples:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Training (80%, ~1,700 samples)"}),": The model learns from these"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Validation (10%, ~210 samples)"}),": Monitors training progress in real-time.","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"In typical ML systems, this is used to tweak hyper parameters. In this case it checks the validation loss during training - and allows you to avoid overfitting, by making sure that training loss doesn\u2019t diverge from validation loss."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Test (10%, ~210 samples)"}),": Final evaluation, never touched during training"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["But before splitting, I ",(0,s.jsx)(n.strong,{children:"shuffle all samples from all conversations"}),". This avoids temporal bias where training data represents one era of thinking while test data represents another."]}),"\n",(0,s.jsx)(n.p,{children:"One of the reasons for which I recommend displaying the number of samples is so that you can put yourself in the shoes of the model to understand how many samples it will see; and that will help you make better decisions in terms of the training and model configs."}),"\n",(0,s.jsx)(o.A,{summary:"Preparing data logs",children:(0,s.jsx)(r.A,{language:"bash",children:"$ python scripts/01_prepare_data.py --config config/data_config.yaml\n\n============================================================\nDATA PREPARATION PIPELINE\n============================================================\n\n>>> Step 1: Loading raw dataset...\n\nLoading dataset: didierlopes/my-blog-qa-dataset\nDataset loaded successfully. Available splits: ['train']\nDataset size: 91 samples\nDataset features: {'title': Value('string'), 'conversation': List({'content': Value('string'), 'role': Value('string')}), 'context': Value('string'), 'url': Value('string'), 'date': Value('string')}\n\n>>> Step 2: Processing and formatting data...\n\nStarting data preprocessing...\nExtracted 2129 conversation samples\nData split created:\nTraining samples: 1705 (80.1%)\nValidation samples: 212 (10.0%)\nTest samples: 212 (10.0%)\nData preprocessing completed successfully!\n\n>>> Step 3: Validating processed data...\n\nValidating training data...\nValidating 10 samples...\n\n\ud83d\udcca Validation Summary:\nTotal samples checked: 10\nValid samples:         10\nInvalid samples:       0\nValidation rate:       100.0%\n\n\u2705 All samples passed validation!\n\nValidating validation data...\nValidating 10 samples...\n\n\ud83d\udcca Validation Summary:\nTotal samples checked: 10\nValid samples:         10\nInvalid samples:       0\nValidation rate:       100.0%\n\n\u2705 All samples passed validation!\n\nValidating test data...\nValidating 10 samples...\n\n\ud83d\udcca Validation Summary:\nTotal samples checked: 10\nValid samples:         10\nInvalid samples:       0\nValidation rate:       100.0%\n\n\u2705 All samples passed validation!\n\nSample statistics:\n\n\ud83d\udcc8 Sample Statistics:\nTotal samples:     2,129\nAverage length:    721 characters\nMin length:        340 characters\nMax length:        3,880 characters\nTotal characters:  1,536,016\n\n>>> Step 4: Saving processed data...\n\nTraining data saved to: data/processed/train.json\nValidation data saved to: data/processed/val.json\nTest data saved to: data/processed/test.json\nData statistics saved to: data/processed/data_stats.json\n\nTraining data: data/processed/train.json\nValidation data: data/processed/val.json\nTest data: data/processed/test.json\nStatistics: data/processed/data_stats.json\nTraining samples: 1705\nValidation samples: 212\nTest samples: 212"})}),"\n",(0,s.jsx)(n.h2,{id:"2-train-the-model",children:"2. Train the model"}),"\n",(0,s.jsxs)(n.p,{children:["Code can be found here: ",(0,s.jsx)(n.a,{href:"https://github.com/DidierRLopes/fine-tune-llm/blob/main/scripts/02_train_model.py",children:"https://github.com/DidierRLopes/fine-tune-llm/blob/main/scripts/02_train_model.py"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"model-configuration",children:"Model configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'  base_model:\n    path: "microsoft/Phi-3-mini-4k-instruct"\n\n  lora:\n    num_layers: 32\n    lora_layers: 32\n    rank: 16\n    scale: 20.0\n    dropout: 0.1\n    keys:\n      - "self_attn.q_proj"\n      - "self_attn.k_proj"\n      - "self_attn.v_proj"\n      - "self_attn.o_proj"\n'})}),"\n",(0,s.jsx)(n.h4,{id:"lora-layers",children:"Lora layers"}),"\n",(0,s.jsxs)(n.p,{children:["From ",(0,s.jsx)(n.a,{href:"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/config.json",children:"phi-3-mini-4k-instruct config file"})," we know that it has 32 hidden layers (i.e. 32 transformer blocks comprised of multi-head self-attention, feed-forward network, and residual connections + layer norms)."]}),"\n",(0,s.jsx)(n.p,{children:"In transformer models like Phi-3, different layers learn different levels of abstraction:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:'Bottom layers (1-10) - "grammar"'}),': Learn fundamental language patterns - grammar, syntax, basic word associations. These layers understand that "CEO" is a noun, that it often precedes "of", and basic sentence structure.']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:'Middle layers (11-20) - "reasoning"'}),': Build conceptual understanding - connecting ideas, understanding context, domain knowledge. These layers learn that "OpenBB" relates to "finance" and "open source", that "Terminal" in my context means a financial analysis tool, not a computer interface.']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:'Top layers (21-32) - "style & expression"'}),': Handle vocabulary selection and style - choosing specific words, maintaining tone, formatting responses. These layers decide whether to say "leverage" vs "use", whether to include technical details, how to structure explanations.']}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Most fine-tuning tutorials suggest only adapting the top 8-16 layers because it's more memory-efficient and often sufficient for simple style transfer. But my writing isn't just about word choice - it's about how I conceptually approach topics:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"When I discuss technical topics, I ground them in practical examples (middle layers)"}),"\n",(0,s.jsx)(n.li,{children:"I have specific patterns of explanation - starting broad, then diving deep (bottom-middle layers)"}),"\n",(0,s.jsx)(n.li,{children:"My sentence construction tends toward clarity over complexity (bottom layers)"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Also, I often change the order of the words due to Portuguese being my primary language - although not correct, ultimately I\u2019m trying to fine-tune a model to represent my writing better. So, in this case, I adapted all 32 layers."}),"\n",(0,s.jsx)(n.p,{children:"Although this happened when I was fine-tuning those 3.1M parameters \ud83d\ude2d"}),"\n",(0,s.jsx)("p",{align:"center",children:(0,s.jsx)("img",{width:"600",src:"/blog/\n2025-09-02-fine-tuning-a-llm-on-my-blog-posts_5.png"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h4,{id:"attention-matrices",children:"Attention matrices"}),"\n",(0,s.jsx)(n.p,{children:"In LoRA, the matrices you can adapt are:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Attention projections"}),": Query, Key, Value, Output","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"This is the most common and impactful, it changes which tokens pay attention to which and how strongly. Ultimately, this is where the reasoning comes from."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feed-forward (MLP) projections"}),": W1, W2","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Changes how representations are transformed nonlinearly - not the most common"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Embeddings & LM head"}),": token embedding matrix and output head","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Almost never adapted"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"We went with the attention projections to adapt the complete attention mechanism. Picking FFN matrices would make adapters 2-3x larger, and I didn\u2019t think it was worth it."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h4,{id:"rank",children:"Rank"}),"\n",(0,s.jsxs)(n.p,{children:["LoRA works by adding to a big weight matrix of dimension ",(0,s.jsx)(n.code,{children:"d x d"}),", and adapted weight matrix of dimension ",(0,s.jsx)(n.code,{children:"(d,d)"})," - where ",(0,s.jsx)(n.code,{children:"d"})," is the hidden_size and can be found in model config, in our case, ",(0,s.jsx)(n.code,{children:"3072"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["So wait, LoRA adds a matrix of size ",(0,s.jsx)(n.code,{children:"3072 \xd7 3072"}),"?"]}),"\n",(0,s.jsx)(n.p,{children:"Yes and no."}),"\n",(0,s.jsxs)(n.p,{children:["It does, but in a smart way - and this is where ",(0,s.jsx)(n.strong,{children:"rank"})," comes into play."]}),"\n",(0,s.jsxs)(n.p,{children:["LoRA adapts two matrices called ",(0,s.jsx)(n.code,{children:"A"})," and ",(0,s.jsx)(n.code,{children:"B"}),", where ",(0,s.jsx)(n.code,{children:"A"})," is of dimension ",(0,s.jsx)(n.code,{children:"(d,r)"})," and ",(0,s.jsx)(n.code,{children:"B"})," is of dimension ",(0,s.jsx)(n.code,{children:"(r,d)"}),". The ",(0,s.jsx)(n.code,{children:"r"})," value is the ",(0,s.jsx)(n.strong,{children:"rank"}),". And although multiplying these matrices results in a matrix of size ",(0,s.jsx)(n.code,{children:"(d,d)"}),", the number of parameters on ",(0,s.jsx)(n.code,{children:"A"})," and ",(0,s.jsx)(n.code,{children:"B"})," combined is ",(0,s.jsx)(n.code,{children:"2dr"}),"."]}),"\n",(0,s.jsx)(n.p,{children:"So,"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Low rank (small r"}),"), LoRA can only make coarse adjustments (cheap, fast)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"High rank (large r)"}),", LoRA can make finer adjustments (better fidelity, but heavier)."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"I actually started with 8 on this one, but results weren\u2019t the best so I doubled it."}),"\n",(0,s.jsx)("p",{align:"center",children:(0,s.jsx)("img",{width:"600",src:"/blog/\n2025-09-02-fine-tuning-a-llm-on-my-blog-posts_6.png"})}),"\n",(0,s.jsxs)(n.p,{children:["Note: The ",(0,s.jsx)(n.code,{children:"AxB"})," multiplication that happens which results in the new ",(0,s.jsx)(n.code,{children:"W"})," matrix adapter is normalized by ",(0,s.jsx)(n.code,{children:"r"}),". This makes it so the update\u2019s magnitude stays roughly stable regardless of rank, otherwise ",(0,s.jsx)(n.code,{children:"r"})," would linearly increase variance of the update."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h4,{id:"scale",children:"Scale"}),"\n",(0,s.jsx)(n.p,{children:"What if you actually wanted there to be higher variance of the update?"}),"\n",(0,s.jsx)(n.p,{children:"As in, you wanted LoRA update to influence even more the frozen weight?"}),"\n",(0,s.jsxs)(n.p,{children:["This is where the ",(0,s.jsx)(n.strong,{children:"scale"})," parameter ",(0,s.jsx)(n.code,{children:"\u03b1"})," comes into play."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Small scale \u03b1"}),": LoRA update has a subtle effect."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Large scale \u03b1"}),": LoRA update dominates more strongly."]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Most PyTorch examples use ",(0,s.jsx)(n.code,{children:"1.0"}),"-",(0,s.jsx)(n.code,{children:"5.0"}),", but MLX's implementation benefits from higher scales - the community suggests ",(0,s.jsx)(n.code,{children:"2 * rank"})," as a rule of thumb, so that LoRA adapters weights don\u2019t get drowned out by the base model frozen weights."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h4,{id:"dropout",children:"Dropout"}),"\n",(0,s.jsxs)(n.p,{children:["Dropout ",(0,s.jsx)(n.code,{children:"p"})," is cheap insurance against overfitting when your fine-tune dataset is narrow (e.g., aligning a model to one person\u2019s writing style) or you have a small dataset (e.g. a few thousand samples). It works by randomly zeroing out parts of the low-rank update with probability ",(0,s.jsx)(n.code,{children:"p"})," (0-1)."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Too much dropout"}),": Slows learning (adapter doesn\u2019t specialize enough)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Too little dropout"}),": Adapter memorizes quirks instead of general style."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"If dataset is huge (not our case lol), often the dropout is skipped altogether since there isn\u2019t the risk of overfitting (I mean it depends on the model a bit, but yea)."}),"\n",(0,s.jsxs)(n.p,{children:["We went with ",(0,s.jsx)(n.code,{children:"0.1"}),", which falls under the recommendation."]}),"\n",(0,s.jsx)(n.h3,{id:"training-configuration",children:"Training Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'training:\n  iters: 2000\n  batch_size: 4\n  learning_rate: 1e-5\n  steps_per_eval: 50\n  grad_checkpoint: true\n  \noptimizer:\n  type: "adam"\n  \nmetrics:\n  patience: 5\n  min_delta: 0.001\n  \npaths:\n  train_data: "data/processed/train.json"\n  test_data: "data/processed/test.json"\n  logs_dir: "logs/training"\n'})}),"\n",(0,s.jsx)(n.p,{children:"The training hyperparameters rationale:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Batch size 4"}),": Larger batches (8-16) provide more stable gradients but require more memory and can miss fine details. Smaller batches (1-2) are noisy. 4 felt like a good sweet spot."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"With ~1,700 training samples and batch size 4, that means that there are 425 steps/epoch. An epoch corresponds to a full pass through the training dataset."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"2000 iterations"}),": With 425 steps per epoch and 2000 total steps, that means that there are roughly 5 epochs."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"This means that the model has seen each of the 1,700 examples ~5 times (in slightly different shuffles)."}),"\n",(0,s.jsx)(n.li,{children:"If you kept training further (10\u201320 epochs), you risk overfitting, memorizing samples instead of generalizing."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Adam"})," stands for ",(0,s.jsx)(n.strong,{children:"Adaptive Momentum Estimation"}),", it is one of the model widely used optimizers in deep learning, and an extension of stochastic gradient descent (SGD). This was an easy choice."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Adaptive - each parameter gets its own learning rate (scaled by gradient history)."}),"\n",(0,s.jsx)(n.li,{children:"Momentum - smooths updates, prevents oscillations."}),"\n",(0,s.jsx)(n.li,{children:"Automatic scaling - no need to tune learning rate schedules as much as vanilla SGD."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Learning rate 1e-5"}),": This controls how big each update step is when adjusting parameters during training. Luckily, there are good rules of thumbs for this value in LoRA / LLM fine-tuning:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Full fine-tunes (big models): 1e-5 to 1e-4"}),"\n",(0,s.jsx)(n.li,{children:"LoRA fine-tunes (small adapters): 1e-5 is a very common sweet spot"}),"\n",(0,s.jsx)(n.li,{children:"If unstable: drop to 5e-6 or even 1e-6"}),"\n",(0,s.jsx)(n.li,{children:"If underfitting (loss barely moving): try 2e-5 or 3e-5"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Evaluation every 50 steps"}),": Frequent enough to catch overfitting early (40 checks across training), but not so frequent that it slows training."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Each evaluation on 210 validation samples takes ~30 seconds."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Gradient checkpointing"})," is a memory-saving technique that reduces GPU usage during training by only storing a subset of intermediate activations in the forward pass. When the backward pass runs, the missing activations are recomputed on the fly, which increases compute time but dramatically lowers memory requirements."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Frees up memory so we can fit a batch size of 4 instead of 2, improving gradient stability."}),"\n",(0,s.jsx)(n.li,{children:"The ~20% slower training is a reasonable trade-off compared to the benefits."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Early stopping (patience 5 and min delta 0.001)"}),": If validation loss doesn't improve by 0.001 for 5 consecutive evaluations (250 iterations), training halts."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"the-training-process",children:"The training process"}),"\n",(0,s.jsx)("p",{align:"center",children:(0,s.jsx)("img",{width:"1000",src:"/blog/\n2025-09-02-fine-tuning-a-llm-on-my-blog-posts_7.png"})}),"\n",(0,s.jsx)(n.p,{children:"Training isn't just about pressing \"run\" and waiting. It's an active process of monitoring, adjusting, and sometimes killing runs that aren't working."}),"\n",(0,s.jsx)(n.p,{children:"Let me walk you through what's actually happening when we fine-tune the model:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Load base Phi-3 model (3.8B parameters)"}),"\n",(0,s.jsx)(n.li,{children:"Freeze base model weights - these never change"}),"\n",(0,s.jsx)(n.li,{children:"Add LoRA adapters (3.1M trainable params, ~0.08% of total)"}),"\n",(0,s.jsx)(n.li,{children:"Train only the adapters using validation for monitoring"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The magic here is that the training only updates the tiny LoRA matrices, but the combined output leverages the full 3.8B parameter knowledge base."}),"\n",(0,s.jsx)(n.p,{children:"During each forward pass, the frozen base model produces its output, and the LoRA adapters add their learned adjustments on top - scaled by a factor of 20.0 to make the adaptations significant enough to matter."}),"\n",(0,s.jsx)(n.p,{children:"In fact, every forward pass, the model processes 4 samples (batch_size), predicts the next tokens, and compares them to the actual tokens. The loss quantifies how wrong it was. Backpropagation then updates only the LoRA weights to reduce this loss."}),"\n",(0,s.jsx)(n.p,{children:"During backpropagation, gradients flow only to the LoRA adapters - the base model parameters never receive gradients and never change."}),"\n",(0,s.jsx)(n.p,{children:"Every 50 steps, I evaluate on validation data."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Training loss dropping, validation loss dropping","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Learning"})," - this is what you want! Model is learning from training data and generalizing to unseen validation data."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Training loss dropping, validation loss flat","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Approaching capacity"}),' - model is learning from the training data, but it\u2019s no longer generalizing better to unseen data. The model is "full" with the representational flexibility given. This is different from underfitting, where the model wouldn\u2019t even be learning more from the training data.']}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Training loss dropping, validation loss rising","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Overfitting"})," - the model is \u201cmemorizing\u201d training data examples. Time to stop, or early stoppage will take care of it."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Both losses flat","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Learning rate too low or model saturated (and we need to go back to tweaking hyperparameters)."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"My training showed steady improvement until iteration ~1,500, where validation loss plateaued around 2.8. The training got halted here due to my patience-based early stoppage (patience 5 - i.e. 250 iterations, min delta 0.001). This saved me from overfitting and wasted compute on iterations 1,500-2,000 where no meaningful learning occurred."}),"\n",(0,s.jsx)(n.p,{children:"Ultimately, this told me the model had extracted what it could from the data."}),"\n",(0,s.jsx)(n.p,{children:"But what does a validation loss of 2.8 mean?"}),"\n",(0,s.jsx)(n.p,{children:"Validation loss of 2.8 translates to a perplexity of ~16.4. This means on average, the model thinks there are about 16 equally likely next tokens at each step."}),"\n",(0,s.jsx)(n.p,{children:"For context, random guessing would be the vocabulary size (e.g. ~32,000) and a perfect model would have 1. So 16.4 is pretty good."}),"\n",(0,s.jsx)(n.p,{children:"Note: The 3.1M parameters aren't learning language from scratch - they're learning how to nudge an already-capable model toward our specific use case. That's why LoRA is so sample-efficient and why we can achieve good results with relatively small datasets."}),"\n",(0,s.jsx)(n.p,{children:"The base model retains all its general knowledge while the LoRA adapters inject domain-specific expertise. It's like having a general practitioner doctor (base model) who takes a specialized course (LoRA training) to better handle specific types of cases, without forgetting their general medical knowledge."}),"\n",(0,s.jsx)(n.p,{children:"Training is fundamentally about finding the right balance: enough learning to improve performance, but not so much that you overfit to your training data. The validation curve is your compass, and early stopping is your safety net."}),"\n",(0,s.jsx)(o.A,{summary:"Training model logs",children:(0,s.jsx)(r.A,{language:"bash",children:"$ python scripts/02_train_model.py --model-config config/model_config.yaml --training-config config/training_config.yaml --train-data data/processed/train.json --val-data data/processed/val.json --test-data data/processed/test.json\n\n============================================================\nMODEL TRAINING PIPELINE\n============================================================\n\nModel config: config/model_config.yaml\nTraining config: config/training_config.yaml\nTraining data: data/processed/train.json\nValidation data: data/processed/val.json\nTest data: data/processed/test.json (for final evaluation only)\n\nMetrics logging to: logs/training\n\n============================================================\nSTARTING FINE-TUNING PIPELINE\n============================================================\n\nLoading training data from: data/processed/train.json\nLoading validation data from: data/processed/val.json\nLoaded 1705 training samples\nLoaded 212 validation samples\nLoading base model: microsoft/Phi-3-mini-4k-instruct\n\nFetching 13 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [00:00<00:00, 46924.23it/s]\n\nModel loaded successfully\n\nLoRA config saved to: models/adapters/adapter_config.json\nFreezing base model parameters...\nConverting linear layers to LoRA layers...\nLoRA setup completed:\nTrainable parameters: 3,145,728 (0.08%)\nTotal parameters:     3,824,225,280\nSetting up adam optimizer with learning rate: 1e-05\n\n============================================================\nTRAINING CONFIGURATION SUMMARY\n============================================================\n\n\ud83d\udcca Dataset:\nTraining samples:   1705\nValidation samples: 212\n\n\ud83d\udd27 LoRA Configuration:\nLayers to adapt:  32/32\nLoRA rank:        16\nLoRA scale:       20.0\nDropout:          0.1\nTarget layers:    self_attn.q_proj, self_attn.k_proj, self_attn.v_proj, self_attn.o_proj\n\n\ud83d\udcc8 Training Parameters:\nIterations:       2000\nBatch size:       4\nLearning rate:    1e-5\nEval frequency:   50\nGrad checkpoint:  True\n\nStarting training...\nTraining on 1705 samples\nValidating on 212 samples\nStarting training..., iters: 2000\nIter 1: Val loss 2.399, Val took 12.519s\nIteration 1: Val loss = 2.3986, Perplexity = 11.01\n\u2192 New best validation loss: 2.3986 (perplexity: 11.01)\nIter 10: Train loss 2.242, Learning Rate 1.000e-05, It/sec 0.805, Tokens/sec 484.578, Trained Tokens 6021, Peak mem 8.924 GB\nIteration 10: Train loss = 2.2423\nIter 20: Train loss 1.522, Learning Rate 1.000e-05, It/sec 0.815, Tokens/sec 487.475, Trained Tokens 12004, Peak mem 8.924 GB\nIteration 20: Train loss = 1.5223\nIter 30: Train loss 1.319, Learning Rate 1.000e-05, It/sec 0.580, Tokens/sec 464.444, Trained Tokens 20014, Peak mem 11.472 GB\nIteration 30: Train loss = 1.3188\n\n(...)\n\n============================================================\nTRAINING SUMMARY\n============================================================\n\n\ud83d\udcca Training Completion:\nTotal iterations:     200\nEarly stopped:        True\nFinal train loss:     0.9613\nFinal val loss:       1.0499\nBest val loss:        1.0393\nFinal perplexity:     2.86\nBest perplexity:      2.83\n\n\u26a0\ufe0f  Training stopped early due to lack of improvement\n Patience counter reached: 24/5\n\n\ud83d\udcc1 Output Files:\nAdapters: models/adapters/\nTraining logs: logs/training/\nMetrics plot: logs/training/training_metrics.png"})}),"\n",(0,s.jsx)(n.h2,{id:"3-evaluate-the-model",children:"3. Evaluate the model"}),"\n",(0,s.jsxs)(n.p,{children:["Code can be found here: ",(0,s.jsx)(n.a,{href:"https://github.com/DidierRLopes/fine-tune-llm/blob/main/scripts/03_evaluate_model.py",children:"https://github.com/DidierRLopes/fine-tune-llm/blob/main/scripts/03_evaluate_model.py"}),"."]}),"\n",(0,s.jsx)(n.p,{children:"The evaluation process answers a critical question:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Did the fine-tuning actually improve the model?"})}),"\n",(0,s.jsx)(n.p,{children:"We compare two models:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Base model"}),": The original Phi-3-mini-4k-instruct (our baseline)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Fine-tuned model"}),": Base model + our trained LoRA adapters applied at runtime"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"step-1-loading-models-for-comparison",children:"Step 1: Loading models for comparison"}),"\n",(0,s.jsx)(n.p,{children:"For the base model evaluation, you need need to load the original model from HF."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'model, tokenizer = load("microsoft/Phi-3-mini-4k-instruct")\n'})}),"\n",(0,s.jsx)(n.p,{children:"For the fine-tuned model evaluation, you need to load the original model from HF AND apply the adapters at runtime."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'model, tokenizer = load("microsoft/Phi-3-mini-4k-instruct") model.freeze()  # Freeze the base weights\nlinear_to_lora_layers(model, lora_config)  # Add LoRA layers\nmodel.load_weights("adapters.safetensors")  # Load trained weights\n'})}),"\n",(0,s.jsx)(n.p,{children:"We're not loading a completely different model file. We're taking the original model and applying the learned adapter weights on top of it at runtime."}),"\n",(0,s.jsx)(n.h3,{id:"step-2-test-data-generation",children:"Step 2: Test data generation"}),"\n",(0,s.jsx)(n.p,{children:"For each test question, both models generate responses."}),"\n",(0,s.jsx)(n.p,{children:'E.g. "What is machine learning?"'}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"base_response = generate(base_model, tokenizer, question, max_tokens=200)\nfinetuned_response = generate(finetuned_model, tokenizer, question, max_tokens=200)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-3-measuring-performance",children:"Step 3: Measuring performance"}),"\n",(0,s.jsx)(n.p,{children:"We utilized world overlap as a simple and interpretable metric. It compares predicted words vs reference answer words. Uses Jaccard similarity: overlap = intersection/union."}),"\n",(0,s.jsx)(n.p,{children:"Example:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'Reference: "Machine learning uses algorithms and data"'}),"\n",(0,s.jsx)(n.li,{children:'Prediction: "ML uses algorithms to learn from data"'}),"\n",(0,s.jsx)(n.li,{children:"Overlap: 4 words match out of 7 unique = 57%"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"A better eval (out-of-scope for this work) would be to implement a version of LMArena where I would ask a question, and get two answers (one from base model and one from fine-tuned) and then I would select which one looked more like my writing without knowing which is which."}),"\n",(0,s.jsx)(o.A,{summary:"Evaluate model logs",children:(0,s.jsx)(r.A,{language:"bash",children:"$ python scripts/03_evaluate_model.py --config config/evaluation_config.yaml --test-data data/processed/test.json --adapters-path models/adapters --base-model microsoft/Phi-3-mini-4k-instruct\n\n============================================================\nMODEL EVALUATION PIPELINE\n============================================================\nStandard evaluation: Base + Runtime LoRA Adapters\n\n\ud83d\udd38 Step 1: Evaluating Base Model\n============================================================\nEVALUATING MODEL: base_model\n============================================================\nLoading model: microsoft/Phi-3-mini-4k-instruct\nFetching 13 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [00:00<00:00, 16008.79it/s]\nModel loaded successfully\nLoading test data from: data/processed/test.json\nExtracted 212 test questions\nGenerating predictions for 212 questions...\nGenerating predictions:   0%|  \n\n(...)\n\nGenerating predictions: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 212/212 [15:24<00:00,  4.36s/it]\nCalculating metrics for 212 samples...\nCalculating word overlap scores...\n\n============================================================\nBASE_MODEL EVALUATION RESULTS\n============================================================\n\n\ud83d\udcca Word Overlap Metrics:\nMean:    0.1574\nMedian:  0.1586\nStd:     0.0443\nRange:   [0.0357, 0.2778]\n\n\ud83d\udccf Length Statistics:\nPredictions: 864.6 chars (\xb1163.1)\nReferences:  405.8 chars (\xb1186.8)\n============================================================\nMetrics saved to: logs/evaluation/base_model_evaluation.json\n\nEvaluation completed! Results saved to: logs/evaluation/base_model_evaluation.json\n\n\ud83d\udd38 Step 2: Evaluating Base Model + Runtime LoRA Adapters\n============================================================\nEVALUATING MODEL: lora_runtime (Base + LoRA Adapters)\n============================================================\nLoading base model: microsoft/Phi-3-mini-4k-instruct\nFetching 13 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [00:00<00:00, 3873.96it/s]\nLoading adapter config: models/adapters/adapter_config.json\nFreezing base model parameters...\nApplying LoRA adapters...\nLoading adapter weights: models/adapters/adapters.safetensors\n\u2705 Model with LoRA adapters loaded successfully\nTrainable parameters: 3,145,728 (0.08%)\nTotal parameters:     3,824,225,280\nLoading test data from: data/processed/test.json\nExtracted 212 test questions\nGenerating predictions for 212 questions...\nGenerating predictions:   0%|      \n\n(...)\n\nGenerating predictions: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 212/212 [07:19<00:00,  2.07s/it]\nCalculating metrics for 212 samples...\nCalculating word overlap scores...\n\n============================================================\nLORA_RUNTIME EVALUATION RESULTS\n============================================================\n\n\ud83d\udcca Word Overlap Metrics:\nMean:    0.2008\nMedian:  0.2025\nStd:     0.0591\nRange:   [0.0449, 0.3913]\n\n\ud83d\udccf Length Statistics:\nPredictions: 381.9 chars (\xb172.9)\nReferences:  405.8 chars (\xb1186.8)\n============================================================\nMetrics saved to: logs/evaluation/lora_runtime_evaluation.json\n\nEvaluation completed! Results saved to: logs/evaluation/lora_runtime_evaluation.json\n\n\ud83d\udd38 Step 3: Model Comparison\n\n============================================================\nMODEL COMPARISON\n============================================================\n\n\ud83d\udcca Model Performance Comparison:\n------------------------------------------------------------\nRank   Model                     Score           Metric         \n------------------------------------------------------------\n1      lora_runtime              0.2008        Word Overlap\n2      base_model                0.1574        Word Overlap\n------------------------------------------------------------\n\n\ud83c\udfaf Best Model (lora_runtime) vs Baseline (base_model):\nScore Improvement: +27.6%\nbase_model: 0.1574\nlora_runtime: 0.2008\n============================================================\nScore comparison plot saved to: logs/evaluation/word_overlap_comparison.png\nScore distribution plot saved to: logs/evaluation/word_overlap_distributions.png\n2025-08-18 01:11:15.187 python3[94401:196290991] The class 'NSSavePanel' overrides the method identifier.  This method is implemented by class 'NSWindow'\n\nComparison report saved to: logs/evaluation/model_comparison_report.json\n\n\ud83d\udcca Detailed comparison plots saved to: logs/evaluation/model_comparison_report.json\n\n============================================================\nEVALUATION PIPELINE COMPLETED!\n============================================================\n\ud83d\udcc1 Output files:\nEvaluation results: logs/evaluation/\nComparison plots: logs/evaluation/\n\n\ud83c\udf89 Evaluation completed! Check the results in the logs directory."})}),"\n",(0,s.jsx)(n.h2,{id:"results-and-statistical-analysis",children:"Results and statistical analysis"}),"\n",(0,s.jsx)("p",{align:"center",children:(0,s.jsx)("img",{width:"800",src:"/blog/\n2025-09-02-fine-tuning-a-llm-on-my-blog-posts_8.png"})}),"\n",(0,s.jsx)(n.p,{children:"The evaluation results tell a compelling story about the effectiveness of our LoRA fine-tuning approach."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"performance-improvement",children:"Performance improvement"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Base model"}),": 0.1574 word overlap (15.74%)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"FT model"}),": 0.2008 word overlap (20.08%)"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Improvement: +27.6% better performance"}),"\n",(0,s.jsx)(n.h3,{id:"consistency-analysis",children:"Consistency analysis"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Base model std dev"}),": 0.0443 (relatively consistent but limited)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"FT std dev"}),": 0.0591 (slightly more variable, but higher overall performance)"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The increased standard deviation in the fine-tuned model actually tells a positive story. While the base model gives consistently mediocre responses, our fine-tuned model shows more range - it's capable of both the baseline performance and significantly better responses."}),"\n",(0,s.jsx)(n.h3,{id:"range-expansion",children:"Range expansion"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Base model range"}),": [0.0357, 0.2778]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"FT range"}),": [0.0449, 0.3913]"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The fine-tuned model's maximum score (0.3913) significantly exceeds the base model's best performance (0.2778). This 41% jump in peak performance shows the model learned to generate responses that better match the reference style and content."}),"\n",(0,s.jsx)(n.h3,{id:"length-analysis",children:"Length analysis"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reference Length"}),": 405.8 characters (\xb1186.8)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Base model"}),": 864.6 characters (\xb1163.1) - verbose, unfocused"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"FT model"}),": 381.9 characters (\xb172.9) - concise, targeted"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This is perhaps the most telling insight. The base model generates responses that are 2.2x longer than the references, suggesting verbose, unfocused answers."}),"\n",(0,s.jsx)(n.p,{children:"The fine-tuned model produces responses much closer to the reference length (381.9 vs 405.8 characters), with lower variability (\xb172.9 vs \xb1163.1), indicating it learned the appropriate response style and length."}),"\n",(0,s.jsx)(n.h3,{id:"parameter-efficiency",children:"Parameter efficiency"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Total parameters: 3.82B"}),"\n",(0,s.jsx)(n.li,{children:"Trainable parameters: 3.15M (0.08%)"}),"\n",(0,s.jsx)(n.li,{children:"Performance gain: +27.6%"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"We achieved nearly 28% better performance by training just 0.08% of the model's parameters. This represents exceptional parameter efficiency - roughly 345% improvement per percentage point of parameters trained."}),"\n",(0,s.jsx)(n.h3,{id:"statistical-interpretation",children:"Statistical interpretation"}),"\n",(0,s.jsx)(n.p,{children:"What 20.08% Word Overlap Actually Means: In a 100-word response, our fine-tuned model shares about 20 words with the reference answer. While this might seem low, it's actually strong performance because:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multiple valid phrasing"}),': "quick" vs "fast", "big" vs "large"']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Different valid approaches"}),": Same facts, different explanations"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Context variations"}),": References are specific examples, predictions are generalizations"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The key insight is relative improvement. The base model achieved 15.74% overlap - our fine-tuning pushed this to 20.08%, crossing an important threshold where responses become noticeably more aligned with expected answers."}),"\n",(0,s.jsx)(n.h2,{id:"vibe-checking",children:"Vibe checking"}),"\n",(0,s.jsxs)(n.p,{children:["Results and statistics are great, but there\u2019s nothing like actually playing with the models and vibe check results. This is why I created ",(0,s.jsx)(n.a,{href:"https://github.com/DidierRLopes/fine-tune-llm/blob/main/scripts/interactive_chat.py",children:"interactive_chat.py file"}),", so that we can ask a question - and we get responses from the base model but also fine-tuned model (and fused if it exists)."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.em,{children:(0,s.jsx)(n.strong,{children:'Prompt: "What is machine learning?"'})})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Base model"}),": ",(0,s.jsx)(n.em,{children:"\"Machine learning is a subset of artificial intelligence (AI) that focuses on the development of computer programs and algorithms that can access data and use it to learn for themselves. In machine learning, algorithms use computational methods to identify patterns in data, learn from those patterns, and then make decisions or predictions based on the data they've studied."})]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.em,{children:"There are three main types of machine learning: supervised learning, unsupervised learning, and reinforcement learning."})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.em,{children:"1. Supervised learning: In this approach, the algorithm is trained on a labeled dataset, meaning that the input data is paired with the desired output. The algorithm learns to map input data to output data based on this training. The goal is to create a model that can accurately predict the output for new, unseen data. Examples of supervised learning include classification (predicting a category or label) and regression (predicting a numerical value)."})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.em,{children:'2. Unsupervised learning"'})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"LoRA runtime (base + adapters)"}),": ",(0,s.jsx)(n.em,{children:"Machine learning is a subfield of artificial intelligence that focuses on the development of algorithms and statistical models to enable computers to learn and improve from data, without being explicitly programmed. It encompasses techniques such as supervised learning, unsupervised learning, and reinforcement learning, which enable systems to make predictions, detect patterns, and make decisions based on large amounts of data."})]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Fused model"}),": ",(0,s.jsx)(n.em,{children:"Machine learning is a field of computer science that focuses on building algorithms capable of learning and improving from data, without explicit programming. It involves developing models that can make predictions or decisions based on input data, often through complex statistical techniques and mathematical optimization."})]}),"\n",(0,s.jsx)(o.A,{summary:"Interactive chat logs",children:(0,s.jsx)(r.A,{language:"bash",children:"$ python scripts/interactive_chat.py\n============================================================\nINTERACTIVE CHAT INTERFACE\n============================================================\nMode: COMPARE\n\n\ud83d\udd38 Loading base model...\nFetching 13 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [00:00<00:00, 51198.08it/s]\n\u2705 Base model loaded\n\n\ud83d\udd38 Loading LoRA runtime (base + adapters)...\nFetching 13 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [00:00<00:00, 38130.04it/s]\n\u2705 LoRA runtime loaded\n\n\ud83d\udd38 Loading fused model...\n\u2705 Fused model loaded\n\n\u2705 Loaded 3 models: ['base', 'lora_runtime', 'fused']\n\n============================================================\nINTERACTIVE COMPARISON MODE\n============================================================\nType your prompts to compare model responses.\nCommands: 'quit' to exit, 'clear' to clear screen\n------------------------------------------------------------\n\n\ud83d\udc64 You: What is machine learning\n\n\ud83e\udd16 Generating from base...\n\ud83e\udd16 Generating from lora_runtime...\n\ud83e\udd16 Generating from fused..."})}),"\n",(0,s.jsx)(n.h2,{id:"model-on-hugging-face",children:"Model on Hugging Face"}),"\n",(0,s.jsxs)(n.p,{children:["Finally, using the ",(0,s.jsx)(n.a,{href:"https://github.com/DidierRLopes/fine-tune-llm/blob/main/scripts/upload_model_to_hf.py",children:"script upload_model_to_hf.py"}),", I have uploaded the model to ",(0,s.jsx)(n.a,{href:"https://huggingface.co/didierlopes/phi-3-mini-4k-instruct-ft-on-my-blog",children:"my Hugging Face repo"}),"."]}),"\n",(0,s.jsx)("p",{align:"center",children:(0,s.jsx)("img",{width:"600",src:"/blog/\n2025-09-02-fine-tuning-a-llm-on-my-blog-posts_9.png"})}),"\n",(0,s.jsx)(o.A,{summary:"Uploading model to hugging face logs",children:(0,s.jsx)(r.A,{language:"bash",children:"$ python scripts/05_upload_model.py --repo-name didierlopes/phi-3-mini-4k-instruct-ft-on-my-blog\n============================================================\nMODEL UPLOAD PIPELINE\n============================================================\nModel path: models/adapters\nModel type: LoRA Adapters\nRepository: didierlopes/phi-3-mini-4k-instruct-ft-on-my-blog\nPrivate: False\nDry run: False\n\nStep 1: Validating model structure...\n\u26a0\ufe0f  Warning: Missing recommended files: ['config.json']\n\u2705 Model validation passed\nFound 21 model weight files\nTotal model size: 252.1 MB\n\nStep 2: Initializing HuggingFace API...\n\u2705 Authenticated as: didierlopes\n\nStep 3: Creating repository...\n\u2705 Repository ready: https://huggingface.co/didierlopes/phi-3-mini-4k-instruct-ft-on-my-blog\n\nStep 4: Creating model card...\n\u2705 Model card created: models/adapters/README.md\n\nStep 5: Uploading model files...\nUploading 23 files...\nProcessing Files (21 / 21)              : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  264MB /  264MB,  441MB/s  \nNew Data Upload                         : |                                                                                                                                                                                                                      |  0.00B /  0.00B,  0.00B/s  \n...apters/0000800_adapters.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12.6MB / 12.6MB            \n...apters/0002000_adapters.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12.6MB / 12.6MB            \n...apters/0000500_adapters.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12.6MB / 12.6MB            \n...apters/0001900_adapters.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12.6MB / 12.6MB            \n...apters/0001500_adapters.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12.6MB / 12.6MB            \n...apters/0000600_adapters.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12.6MB / 12.6MB            \n...apters/0000700_adapters.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12.6MB / 12.6MB            \n...apters/0001000_adapters.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12.6MB / 12.6MB            \n...apters/0001100_adapters.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12.6MB / 12.6MB            \n...apters/0000900_adapters.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12.6MB / 12.6MB            \nNo files have been modified since last commit. Skipping to prevent empty commit.\n\u2705 Upload completed successfully!\n\nStep 6: Verifying upload...\n\u2705 Upload verified: 24 files in repository\n\n============================================================\nMODEL UPLOAD COMPLETED SUCCESSFULLY!\n============================================================\n\ud83c\udf89 Model uploaded to: https://huggingface.co/didierlopes/phi-3-mini-4k-instruct-ft-on-my-blog"})}),"\n",(0,s.jsx)(n.h2,{id:"fused-model",children:"Fused model"}),"\n",(0,s.jsxs)(n.p,{children:["Code can be found here: ",(0,s.jsx)(n.a,{href:"https://github.com/DidierRLopes/fine-tune-llm/blob/main/scripts/04_fuse_and_evaluate.py",children:"https://github.com/DidierRLopes/fine-tune-llm/blob/main/scripts/04_fuse_and_evaluate.py"}),"."]}),"\n",(0,s.jsx)(n.p,{children:"While LoRA adapters are excellent for experimentation and sharing, production deployments often benefit from a single, unified model file. This is where fusion comes in."}),"\n",(0,s.jsx)("p",{align:"center",children:(0,s.jsx)("img",{width:"400",src:"/blog/\n2025-09-02-fine-tuning-a-llm-on-my-blog-posts_10.png"})}),"\n",(0,s.jsx)(n.p,{children:"Fusion mathematically merges your LoRA adapter weights back into the base model. It is recommended to use the fused model when:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Deploying to production"}),"\n",(0,s.jsx)(n.li,{children:"Inference speed is critical"}),"\n",(0,s.jsx)(n.li,{children:"You want a single model file"}),"\n",(0,s.jsx)(n.li,{children:"Sharing with users who aren't familiar with LoRA"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"It also uses less memory, since you have a single model instead of a model + its adapters."}),"\n",(0,s.jsx)(n.p,{children:"However, keeping adapters may still be important, particularly when"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Experimenting with different configurations","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"E.g. if you wanted to fine tune based on your blogs"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Storage/bandwidth is limited","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"This is why I pushed the adapters to HF and not the fused model"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.li,{children:"You need to swap between multiple fine-tuned versions"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:'Note: Fusion is a one-way operation. Once fused, you can\'t extract the adapters back out. Always keep your original adapter files as your "source of truth". Think of the fused model as a compiled binary, and your adapters as the source code.'}),"\n",(0,s.jsx)(o.A,{summary:"Fused model logs",children:(0,s.jsx)(r.A,{language:"bash",children:"$ python scripts/04_fuse_and_evaluate.py \n============================================================\nFUSION AND COMPREHENSIVE EVALUATION PIPELINE\n============================================================\nThis script: 1) Fuses adapters  2) Evaluates Base + Runtime + Fused\nBase model: microsoft/Phi-3-mini-4k-instruct\nAdapters: models/adapters\nFused output: models/fused\nTest data: data/processed/test.json\n\n============================================================\nSTEP 1: ADAPTER FUSION\n============================================================\n\u2705 Fusion inputs validated successfully\n============================================================\nFUSING LORA ADAPTERS\n============================================================\nBase model: microsoft/Phi-3-mini-4k-instruct\nAdapters: models/adapters\nOutput: models/fused\nCommand: mlx_lm.fuse --model microsoft/Phi-3-mini-4k-instruct --adapter-path models/adapters --save-path models/fused\n------------------------------------------------------------\nLoading pretrained model\nFetching 13 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [00:00<00:00, 15029.20it/s]\n------------------------------------------------------------\n\u2705 Fusion completed successfully!\nFused model saved to: models/fused\n============================================================\n\u2705 Fusion completed: models/fused\n\n============================================================\nSTEP 2: COMPREHENSIVE EVALUATION\n============================================================\nEvaluating: Base + Runtime LoRA + Fused\n\n================================================================================\nCOMPREHENSIVE MODEL COMPARISON\nBase Model vs Runtime LoRA vs Fused Model\n================================================================================\n\n\ud83d\udd38 Step 1: Evaluating Base Model\n============================================================\nEVALUATING MODEL: base_model\n============================================================\nLoading model: microsoft/Phi-3-mini-4k-instruct\nFetching 13 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [00:00<00:00, 16881.10it/s]\nModel loaded successfully\nLoading test data from: data/processed/test.json\nExtracted 212 test questions\nGenerating predictions for 212 questions...\nGenerating predictions:   0%| \n\n(...)\n\nGenerating predictions: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 212/212 [15:08<00:00,  4.29s/it]\nCalculating metrics for 212 samples...\nCalculating word overlap scores...\n\n============================================================\nBASE_MODEL EVALUATION RESULTS\n============================================================\n\n\ud83d\udcca Word Overlap Metrics:\nMean:    0.1563\nMedian:  0.1532\nStd:     0.0468\nRange:   [0.0000, 0.3333]\n\n\ud83d\udccf Length Statistics:\nPredictions: 866.8 chars (\xb1154.2)\nReferences:  405.8 chars (\xb1186.8)\n============================================================\nMetrics saved to: logs/evaluation/base_model_evaluation.json\n\nEvaluation completed! Results saved to: logs/evaluation/base_model_evaluation.json\n\n\ud83d\udd38 Step 2: Evaluating Base Model + Runtime LoRA Adapters\n============================================================\nEVALUATING MODEL: lora_runtime (Base + LoRA Adapters)\n============================================================\nLoading base model: microsoft/Phi-3-mini-4k-instruct\nFetching 13 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [00:00<00:00, 13977.43it/s]\nLoading adapter config: models/adapters/adapter_config.json\nFreezing base model parameters...\nApplying LoRA adapters...\nLoading adapter weights: models/adapters/adapters.safetensors\n\u2705 Model with LoRA adapters loaded successfully\nTrainable parameters: 3,145,728 (0.08%)\nTotal parameters:     3,824,225,280\nLoading test data from: data/processed/test.json\nExtracted 212 test questions\nGenerating predictions for 212 questions...\nGenerating predictions:   0%|             \n\n(...)\n\nGenerating predictions: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 212/212 [07:07<00:00,  2.02s/it]\nCalculating metrics for 212 samples...\nCalculating word overlap scores...\n\n============================================================\nLORA_RUNTIME EVALUATION RESULTS\n============================================================\n\n\ud83d\udcca Word Overlap Metrics:\nMean:    0.1940\nMedian:  0.1847\nStd:     0.0627\nRange:   [0.0658, 0.3846]\n\n\ud83d\udccf Length Statistics:\nPredictions: 377.1 chars (\xb170.4)\nReferences:  405.8 chars (\xb1186.8)\n============================================================\nMetrics saved to: logs/evaluation/lora_runtime_evaluation.json\n\nEvaluation completed! Results saved to: logs/evaluation/lora_runtime_evaluation.json\n\n\ud83d\udd38 Step 3: Evaluating Fused Model\n============================================================\nEVALUATING MODEL: lora_fused\n============================================================\nLoading model: models/fused\nModel loaded successfully\nLoading test data from: data/processed/test.json\nExtracted 212 test questions\nGenerating predictions for 212 questions...\nGenerating predictions:   0%|  \n\n(...)\n\nGenerating predictions: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 212/212 [06:23<00:00,  1.81s/it]\nCalculating metrics for 212 samples...\nCalculating word overlap scores...\n\n============================================================\nLORA_FUSED EVALUATION RESULTS\n============================================================\n\n\ud83d\udcca Word Overlap Metrics:\nMean:    0.2027\nMedian:  0.2000\nStd:     0.0638\nRange:   [0.0690, 0.4521]\n\n\ud83d\udccf Length Statistics:\nPredictions: 379.1 chars (\xb161.8)\nReferences:  405.8 chars (\xb1186.8)\n============================================================\nMetrics saved to: logs/evaluation/lora_fused_evaluation.json\n\nEvaluation completed! Results saved to: logs/evaluation/lora_fused_evaluation.json\n\n\ud83d\udd38 Step 4: Model Comparison\n\n============================================================\nMODEL COMPARISON\n============================================================\n\n\ud83d\udcca Model Performance Comparison:\n------------------------------------------------------------\nRank   Model                     Score           Metric         \n------------------------------------------------------------\n1      lora_fused                0.2027        Word Overlap\n2      lora_runtime              0.1940        Word Overlap\n3      base_model                0.1563        Word Overlap\n------------------------------------------------------------\n\n\ud83c\udfaf Best Model (lora_fused) vs Baseline (base_model):\nScore Improvement: +29.7%\nbase_model: 0.1563\nlora_fused: 0.2027\n============================================================\n\n\ud83d\udd38 Step 5: Fusion Verification\n\n============================================================\nFUSION QUALITY VERIFICATION\n============================================================\n\ud83d\udcca Word Overlap Comparison:\nRuntime Adapters: 0.1940\nFused Model:      0.2027\nAbsolute Diff:    0.0086\nRelative Diff:    4.45%\n\u26a0\ufe0f  Acceptable fusion quality - some degradation detected\n\n\ud83d\udcc8 Additional Metrics Comparison:\nWord Overlap Std Dev: Runtime 0.0627 | Fused 0.0638\nWord Overlap Range:   Runtime [0.066, 0.385] | Fused [0.069, 0.452]\n============================================================\n\n\ud83d\udcc1 Comprehensive results saved to: logs/evaluation/comprehensive_comparison_20250818_021105.json\n\n============================================================\nPIPELINE COMPLETED SUCCESSFULLY!\n============================================================\n\n\ud83d\udcca Models Evaluated: base_model, lora_runtime, lora_fused\n\n\ud83c\udfaf Key Results:\nbase_model: 0.1563 (Word Overlap)\nlora_runtime: 0.1940 (Word Overlap)\nlora_fused: 0.2027 (Word Overlap)\n\n\ud83d\udcc1 Detailed results saved to: logs/evaluation/\n\ud83d\udcc1 Fused model available at: models/fused"})}),"\n",(0,s.jsx)(n.h2,{id:"wrap-up",children:"Wrap up"}),"\n",(0,s.jsx)(n.p,{children:"And that\u2019s it!"}),"\n",(0,s.jsx)(n.p,{children:"The results demonstrate that LoRA fine-tuning achieved its goal: meaningful performance improvement with minimal computational overhead, faster inference, and more focused responses that better match the target domain."}),"\n",(0,s.jsxs)(n.p,{children:["I hope you find this helpful, all the code can be found here: ",(0,s.jsx)(n.a,{href:"https://github.com/DidierRLopes/fine-tune-llm",children:"https://github.com/DidierRLopes/fine-tune-llm"}),"."]})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(p,{...e})}):p(e)}},88897:e=>{e.exports=JSON.parse('{"permalink":"/blog/fine-tuning-a-llm-on-my-blog-posts","editUrl":"https://github.com/DidierRLopes/my-website/tree/main/blog/2025-09-02-fine-tuning-a-llm-on-my-blog-posts.md","source":"@site/blog/2025-09-02-fine-tuning-a-llm-on-my-blog-posts.md","title":"Fine-tuning a LLM on my blog posts","description":"Ever wondered what it would be like to have an AI that writes exactly in your style? I did. And in this post, I share what I did about it. This is a very practical guide on how to fine-tune an LLM using LoRA with MLX on Apple Silicon.","date":"2025-09-02T00:00:00.000Z","tags":[{"inline":true,"label":"ai","permalink":"/blog/tags/ai"},{"inline":true,"label":"machine-learning","permalink":"/blog/tags/machine-learning"},{"inline":true,"label":"open-source","permalink":"/blog/tags/open-source"},{"inline":true,"label":"tutorial","permalink":"/blog/tags/tutorial"},{"inline":true,"label":"apple-silicon","permalink":"/blog/tags/apple-silicon"},{"inline":true,"label":"llm","permalink":"/blog/tags/llm"},{"inline":true,"label":"fine-tuning","permalink":"/blog/tags/fine-tuning"}],"readingTime":34.58,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"fine-tuning-a-llm-on-my-blog-posts","title":"Fine-tuning a LLM on my blog posts","date":"2025-09-02T00:00:00.000Z","image":"/blog/2025-09-02-fine-tuning-a-llm-on-my-blog-posts","tags":["ai","machine-learning","open-source","tutorial","apple-silicon","llm","fine-tuning"],"description":"Ever wondered what it would be like to have an AI that writes exactly in your style? I did. And in this post, I share what I did about it. This is a very practical guide on how to fine-tune an LLM using LoRA with MLX on Apple Silicon.","hideSidebar":true},"unlisted":false,"prevItem":{"title":"Tracking your partner\'s health stats with Oura and AI","permalink":"/blog/tracking-your-partner-health-stats-with-oura-and-ai"},"nextItem":{"title":"Top 10 OpenBB apps","permalink":"/blog/top-10-openbb-apps"}}')}}]);