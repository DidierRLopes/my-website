<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">Fine-tuning a LLM on my blog posts | Didier Lopes</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://didierlopes.com/blog/2025-09-02-fine-tuning-a-llm-on-my-blog-posts"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Fine-tuning a LLM on my blog posts | Didier Lopes"><meta data-rh="true" name="description" content="Ever wondered what it would be like to have an AI that writes exactly in your style? I did. And in this post, I share what I did about it. This is a very practical guide on how to fine-tune an LLM using LoRA with MLX on Apple Silicon."><meta data-rh="true" property="og:description" content="Ever wondered what it would be like to have an AI that writes exactly in your style? I did. And in this post, I share what I did about it. This is a very practical guide on how to fine-tune an LLM using LoRA with MLX on Apple Silicon."><meta data-rh="true" property="og:image" content="https://didierlopes.com/blog/2025-09-02-fine-tuning-a-llm-on-my-blog-posts"><meta data-rh="true" name="twitter:image" content="https://didierlopes.com/blog/2025-09-02-fine-tuning-a-llm-on-my-blog-posts"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2025-09-02T00:00:00.000Z"><meta data-rh="true" property="article:tag" content="ai,machine-learning,open-source,tutorial,apple-silicon,llm,fine-tuning"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://didierlopes.com/blog/2025-09-02-fine-tuning-a-llm-on-my-blog-posts"><link data-rh="true" rel="alternate" href="https://didierlopes.com/blog/2025-09-02-fine-tuning-a-llm-on-my-blog-posts" hreflang="en"><link data-rh="true" rel="alternate" href="https://didierlopes.com/blog/2025-09-02-fine-tuning-a-llm-on-my-blog-posts" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://CTGM87XQE8-dsn.algolia.net" crossorigin="anonymous"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","@id":"https://didierlopes.com/blog/2025-09-02-fine-tuning-a-llm-on-my-blog-posts","mainEntityOfPage":"https://didierlopes.com/blog/2025-09-02-fine-tuning-a-llm-on-my-blog-posts","url":"https://didierlopes.com/blog/2025-09-02-fine-tuning-a-llm-on-my-blog-posts","headline":"Fine-tuning a LLM on my blog posts","name":"Fine-tuning a LLM on my blog posts","description":"Ever wondered what it would be like to have an AI that writes exactly in your style? I did. And in this post, I share what I did about it. This is a very practical guide on how to fine-tune an LLM using LoRA with MLX on Apple Silicon.","datePublished":"2025-09-02T00:00:00.000Z","author":[],"image":{"@type":"ImageObject","@id":"https://didierlopes.com/blog/2025-09-02-fine-tuning-a-llm-on-my-blog-posts","url":"https://didierlopes.com/blog/2025-09-02-fine-tuning-a-llm-on-my-blog-posts","contentUrl":"https://didierlopes.com/blog/2025-09-02-fine-tuning-a-llm-on-my-blog-posts","caption":"title image for the blog post: Fine-tuning a LLM on my blog posts"},"keywords":[],"isPartOf":{"@type":"Blog","@id":"https://didierlopes.com/blog","name":"Blog"}}</script><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Didier Lopes RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Didier Lopes Atom Feed">
<link rel="alternate" type="application/json" href="/blog/feed.json" title="Didier Lopes JSON Feed">




<link rel="search" type="application/opensearchdescription+xml" title="Didier Lopes" href="/opensearch.xml">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script>window.dataLayer=window.dataLayer||[]</script>
<script>!function(e,t,a,n){e[n]=e[n]||[],e[n].push({"gtm.start":(new Date).getTime(),event:"gtm.js"});var g=t.getElementsByTagName(a)[0],m=t.createElement(a);m.async=!0,m.src="https://www.googletagmanager.com/gtm.js?id=GTM-PL77JR5L",g.parentNode.insertBefore(m,g)}(window,document,"script","dataLayer")</script>


<link rel="stylesheet" href="src/css/custom.css"><link rel="stylesheet" href="/assets/css/styles.c0002ebf.css">
<script src="/assets/js/runtime~main.2645cd45.js" defer="defer"></script>
<script src="/assets/js/main.2649c2d1.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="dark";var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script>

<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PL77JR5L" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" target="_self" href="/"><div class="navbar__logo"><img src="/img/goku_pixel.png" alt="Didier Lopes logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/goku_pixel.png" alt="Didier Lopes logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Didier Lopes</b></a><div class="navbar__item"><div class="navbar__items--left-wrapper"></div><a class="navbar__item navbar__link" href="/books/to-read">Books</a><a class="navbar__item navbar__link" href="/media/on-stage">Media</a><a class="navbar__item navbar__link" href="/resume/experience">Resume</a><div class="navbar__item"></div></div><a aria-current="page" class="navbar__item navbar__link navbar__center-item navbar__link--active" href="/blog">Blog</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/intelligence">Intelligence</a><a class="navbar__item navbar__link" href="/chat">Chat</a><a href="/newsletter" class="navbar__item navbar__link navbar-item-external navbar-newsletter-link" target="_blank" rel="noopener noreferrer">Newsletter</a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><main class="col col--9 col--offset-1"><div><article class=""><header><h1 class="title_f1Hy">Fine-tuning a LLM on my blog posts</h1><div class="container_mt6G margin-vert--md"><time datetime="2025-09-02T00:00:00.000Z">September 2, 2025</time> ¬∑ <!-- -->35 min read</div></header><div id="__blog-post-container" class="markdown"><p align="center"><img width="600" src="/blog/
2025-09-02-fine-tuning-a-llm-on-my-blog-posts.png"></p>
<p>Ever wondered what it would be like to have an AI that writes exactly in your style? I did. And in this post, I share what I did about it. This is a very practical guide on how to fine-tune an LLM using LoRA with MLX on Apple Silicon.</p>
<!-- -->
<div style="border-top:1px solid #0088CC;margin:1.5em 0"></div>
<p>At the start of the year I shared <a href="https://didierlopes.com/blog/turn-my-blog-feed-into-a-qa-dataset-to-fine-tune-a-llm" target="_blank" rel="noopener noreferrer">this blogpost</a>, which converted all my blogs into a Q&amp;A dataset that I could use to fine-tune a LLM.</p>
<p align="center"><img width="600" src="/blog/
2025-09-02-fine-tuning-a-llm-on-my-blog-posts_1.png"></p>
<p>After sharing this, I spent time trying to fine-tune an LLM - but results were not great and combined with all things happening at OpenBB I didn&#x27;t have time to dedicate a lot of time to this. <!-- -->:upside-down-hf-logo</p>
<p>But I hate leaving things half way. And this task didn‚Äôt leave my TODO for the past 6 months.</p>
<p>So I finally took things to my hands last weekend, and I‚Äôm going to share the entire journey on what, why and how.</p>
<p>Buckle up, this will be a long post - and more technical than previous ones. And all the code will be available here: <a href="https://github.com/DidierRLopes/fine-tune-llm" target="_blank" rel="noopener noreferrer">https://github.com/DidierRLopes/fine-tune-llm</a>.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="context">Context<a href="#context" class="hash-link" aria-label="Direct link to Context" title="Direct link to Context">‚Äã</a></h2>
<p>Most AI models are like Wikipedia - they know a little about everything but lack the depth and personality that comes from lived experience.</p>
<p>Think of it this way: RAG is like giving someone a reference book during an exam. Fine-tuning is like actually teaching them the subject until it becomes part of how they think.</p>
<blockquote>
<p><em>‚ÄúOnce you‚Äôve maximized the performance gains from prompting, you might wonder whether to do RAG or finetuning next. The answer depends on whether your model‚Äôs failures are information-based or behavior-based.</em></p>
<p><em>If the model fails because it lacks information, a RAG syustem that gives the model access to the relevant sources of information can help. (‚Ä¶) On the other hand, if the model has behavioral issues, finetuning might help.‚Äù</em></p>
<br>
<p>- Chip Huyen‚Äôs - AI Engineering (Chapter 7: Finetuning)</p>
</blockquote>
<br>
<p>When you fine-tune a model on your writing, you&#x27;re not just feeding it information (particularly with small models and a LoRA - you&#x27;re rewiring how it processes and responds to ideas. The same neural pathways that learned to write about quantum physics now learn your specific way of sharing thoughts on open source, MCP, boxing, and others.</p>
<p>In this case, because we will fine-tune an instruct model - even the system prompt becomes part of this personalization process from the very first token. It‚Äôs not a simple ‚ÄúYou are a helpful assistant&quot; but ‚ÄúYou are Didier, CEO of OpenBB. You write with clarity and impact, focusing on fintech, open source, AI, and the future of research workflows‚Äù.</p>
<p>This will result in a fine-tuned model that thinks in your voice and operates with your expertise baseline. To some extent that is, we will see later that the information transfer could be better. I attribute that to the fact that we are using a small (3.8B model), we are doing partial fine-tuning (only 0.08% of weights will be updated) and I didn‚Äôt spend a lot of time iterating on the hyperparameters.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="0-setting-up-the-foundation">0. Setting up the foundation<a href="#0-setting-up-the-foundation" class="hash-link" aria-label="Direct link to 0. Setting up the foundation" title="Direct link to 0. Setting up the foundation">‚Äã</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="model">Model<a href="#model" class="hash-link" aria-label="Direct link to Model" title="Direct link to Model">‚Äã</a></h3>
<p>I chose <strong>Microsoft&#x27;s Phi-3 mini model (3.8B parameters)</strong> for several strategic reasons beyond just &quot;it fits on my Mac&quot;:</p>
<p><strong>Technical sweet spot</strong>: At 3.8B parameters, Phi-3 mini hits the perfect balance - large enough to produce coherent, contextual responses, but small enough to fine-tune efficiently on consumer hardware. Larger models like 7B+ would require more aggressive quantization.</p>
<p><strong>Instruct-optimized foundation</strong>: This isn&#x27;t a raw base model. Phi-3 mini is already instruction-tuned with supervised fine-tuning (SFT) and likely RLHF, meaning it understands how to follow prompts and maintain conversational flow. This gives me a much better starting point than training from a base model. Note: Microsoft actually did not release the base model.</p>
<p><strong>Ecosystem support</strong>:</p>
<ul>
<li>This <a href="https://gist.github.com/andrewssobral/89ca0cd40e609a32c0ce8241d01f484d" target="_blank" rel="noopener noreferrer">code reference</a> gave me a working starting point</li>
<li>There was an <a href="https://github.com/microsoft/PhiCookBook/blob/main/md/03.FineTuning/FineTuning_MLX.md" target="_blank" rel="noopener noreferrer">official cookbook</a> with best practices</li>
<li>There was a good <a href="https://huggingface.co/microsoft/Phi-3-mini-4k-instruct" target="_blank" rel="noopener noreferrer">model card on Hugging Face</a> with clear usage example</li>
</ul>
<br>
<p><strong>Hardware compatibility</strong>: With my M3 Max and 48GB RAM, this model fits comfortably in memory with room for LoRA adapters and training overhead.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="finetuning-technique">Finetuning Technique<a href="#finetuning-technique" class="hash-link" aria-label="Direct link to Finetuning Technique" title="Direct link to Finetuning Technique">‚Äã</a></h3>
<p>Traditional fine-tuning updates all 3.8 billion parameters, requiring enormous compute resources and risking catastrophic forgetting (where the model loses its general capabilities while learning your specific data).</p>
<p><strong>LoRA&#x27;s elegant solution</strong>: Low-Rank Adaptation works by decomposing weight updates into smaller matrices. Instead of modifying a large weight matrix W directly, LoRA adds two smaller matrices <code>A</code> and <code>B</code> such that the update becomes <code>W + BA</code>, where <code>B</code> has rank <code>r</code> and <code>r &lt;&lt; d</code> (with <code>d</code> being the original dimensions). More on LoRA <a href="https://huggingface.co/docs/peft/developer_guides/lora" target="_blank" rel="noopener noreferrer">here</a>.</p>
<p align="center"><img width="600" src="/blog/
2025-09-02-fine-tuning-a-llm-on-my-blog-posts_2.png"></p>
<p>Why this matters:</p>
<ul>
<li><strong>Parameter efficiency</strong>: I&#x27;m only training a small percentage (<code>&lt;0.2%</code>) of the entire 3.8b model</li>
<li><strong>Memory efficiency</strong>: Base model stays frozen, only adapter weights need gradients</li>
<li><strong>Modularity</strong>: Can swap different LoRA adapters for different tasks/personalities</li>
<li><strong>Reduced overfitting</strong>: Smaller parameter space makes it harder to memorize training data (which also validates the fact that fine-tuning is not best choice to give more information to a model)</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="framework">Framework<a href="#framework" class="hash-link" aria-label="Direct link to Framework" title="Direct link to Framework">‚Äã</a></h3>
<p>MLX is specifically designed for Apple&#x27;s unified memory architecture. While PyTorch can run on Mac, it wasn&#x27;t built with Apple Silicon&#x27;s unique characteristics in mind.</p>
<p>Key MLX benefits:</p>
<ul>
<li><strong>Memory efficiency</strong>: Unified memory means no CPU/GPU transfers, LoRA adapters and base model share the same memory pool efficiently</li>
<li><strong>Lazy evaluation</strong>: Only computes what&#x27;s needed, when it&#x27;s needed - crucial for memory-constrained fine-tuning</li>
<li><strong>Native optimization</strong>: Built for Apple&#x27;s AMX (Apple Matrix Extensions) and Neural Engine integration</li>
</ul>
<p>Most production fine-tuning still happens on NVIDIA GPUs with PyTorch. But for Apple Silicon users, MLX offers several advantages:</p>
<ul>
<li><strong>Lower barrier to entry</strong>: No need for cloud GPUs or expensive NVIDIA hardware</li>
<li><strong>Rapid experimentation</strong>: Faster iteration cycles for smaller models</li>
<li><strong>Privacy</strong>: Everything runs locally, no data leaves your machine</li>
</ul>
<p>Note that I was able to do this because I was working with a &lt;10B parameter model and had Apple Silicon with 48GB RAM. But more importantly, this was done for experimentation, and not production - so I chose what allowed me to get my hands dirty faster.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="1-preparing-the-data">1. Preparing the data<a href="#1-preparing-the-data" class="hash-link" aria-label="Direct link to 1. Preparing the data" title="Direct link to 1. Preparing the data">‚Äã</a></h2>
<p>Code can be found here: <a href="https://github.com/DidierRLopes/fine-tune-llm/blob/main/scripts/01_prepare_data.py" target="_blank" rel="noopener noreferrer">https://github.com/DidierRLopes/fine-tune-llm/blob/main/scripts/01_prepare_data.py</a>.</p>
<p>For the data we will be using a <a href="https://huggingface.co/datasets/didierlopes/my-blog-qa-dataset" target="_blank" rel="noopener noreferrer">Q&amp;A dataset based on my blogposts</a>. The repository where I turned my blog posts into this dataset can be found <a href="https://github.com/DidierRLopes/turn-blog-feed-into-qa-dataset/tree/main" target="_blank" rel="noopener noreferrer">here</a>.</p>
<p align="center"><img width="600" src="/blog/
2025-09-02-fine-tuning-a-llm-on-my-blog-posts_3.png"></p>
<p>The dataset contains 91 blog posts transformed into conversational Q&amp;A pairs - roughly 2,100 exchanges covering everything from OpenBB&#x27;s journey to technical deep-dives on open source.</p>
<p>Each entry in the dataset contains conversations with user questions and my responses. But raw conversational data (which I parsed from a blogpost) isn&#x27;t something you can just throw at a model. It needs structure, and more importantly, <strong>it needs the right structure</strong> for your chosen model.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="formatting-for-phi-3-mini-4k-instruct">Formatting for phi-3-mini-4k-instruct<a href="#formatting-for-phi-3-mini-4k-instruct" class="hash-link" aria-label="Direct link to Formatting for phi-3-mini-4k-instruct" title="Direct link to Formatting for phi-3-mini-4k-instruct">‚Äã</a></h3>
<p>Phi-3-mini-4k-instruct has been trained with a specific chat template, and we need to follow it - otherwise results won&#x27;t be optimal (this was one of my first mistakes!)</p>
<p align="center"><img width="600" src="/blog/
2025-09-02-fine-tuning-a-llm-on-my-blog-posts_4.png"></p>
<p>You can find that template in the model card on HF: <a href="https://huggingface.co/microsoft/Phi-3-mini-4k-instruct" target="_blank" rel="noopener noreferrer">https://huggingface.co/microsoft/Phi-3-mini-4k-instruct</a></p>
<p>Important: Since this is an instruct model, then it is important to retain the system prompt on the training samples. (I also did a mistake here!)</p>
<p>Example:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">&lt;|system|&gt;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">You are a helpful assistant.&lt;|end|&gt;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">&lt;|user|&gt;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">How to explain Internet for a medieval knight?&lt;|end|&gt;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">&lt;|assistant|&gt;</span><br></span></code></pre></div></div>
<p>Those special tokens (<code>&lt;|system|&gt;</code>, <code>&lt;|user|&gt;</code>, <code>&lt;|assistant|&gt;</code>, <code>&lt;|end|&gt;</code>) aren&#x27;t decorative, they&#x27;re semantic markers that tell the model exactly where each part of the conversation begins and ends. (Do not forget these, and ensure there are no typos! I did not do a mistake here ehe)</p>
<p>I actually added a function to validate if the required tokens existed, and are in the right order.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="training-split">Training split<a href="#training-split" class="hash-link" aria-label="Direct link to Training split" title="Direct link to Training split">‚Äã</a></h3>
<p>One of the most common mistakes in fine-tuning is treating your test data as validation data. Here&#x27;s how I split the ~2,100 samples:</p>
<ul>
<li><strong>Training (80%, ~1,700 samples)</strong>: The model learns from these</li>
<li><strong>Validation (10%, ~210 samples)</strong>: Monitors training progress in real-time.<!-- -->
<ul>
<li>In typical ML systems, this is used to tweak hyper parameters. In this case it checks the validation loss during training - and allows you to avoid overfitting, by making sure that training loss doesn‚Äôt diverge from validation loss.</li>
</ul>
</li>
<li><strong>Test (10%, ~210 samples)</strong>: Final evaluation, never touched during training</li>
</ul>
<p>But before splitting, I <strong>shuffle all samples from all conversations</strong>. This avoids temporal bias where training data represents one era of thinking while test data represents another.</p>
<p>One of the reasons for which I recommend displaying the number of samples is so that you can put yourself in the shoes of the model to understand how many samples it will see; and that will help you make better decisions in terms of the training and model configs.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Preparing data logs</summary><div><div class="collapsibleContent_i85q"><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">$ python scripts/01_prepare_data.py --config config/data_config.yaml</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">DATA PREPARATION PIPELINE</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">&gt;&gt;&gt; Step 1: Loading raw dataset...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loading dataset: didierlopes/my-blog-qa-dataset</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Dataset loaded successfully. Available splits: [&#x27;train&#x27;]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Dataset size: 91 samples</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Dataset features: {&#x27;title&#x27;: Value(&#x27;string&#x27;), &#x27;conversation&#x27;: List({&#x27;content&#x27;: Value(&#x27;string&#x27;), &#x27;role&#x27;: Value(&#x27;string&#x27;)}), &#x27;context&#x27;: Value(&#x27;string&#x27;), &#x27;url&#x27;: Value(&#x27;string&#x27;), &#x27;date&#x27;: Value(&#x27;string&#x27;)}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">&gt;&gt;&gt; Step 2: Processing and formatting data...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Starting data preprocessing...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Extracted 2129 conversation samples</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Data split created:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Training samples: 1705 (80.1%)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Validation samples: 212 (10.0%)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Test samples: 212 (10.0%)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Data preprocessing completed successfully!</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">&gt;&gt;&gt; Step 3: Validating processed data...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Validating training data...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Validating 10 samples...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üìä Validation Summary:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Total samples checked: 10</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Valid samples:         10</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Invalid samples:       0</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Validation rate:       100.0%</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">‚úÖ All samples passed validation!</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Validating validation data...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Validating 10 samples...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üìä Validation Summary:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Total samples checked: 10</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Valid samples:         10</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Invalid samples:       0</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Validation rate:       100.0%</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">‚úÖ All samples passed validation!</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Validating test data...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Validating 10 samples...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üìä Validation Summary:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Total samples checked: 10</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Valid samples:         10</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Invalid samples:       0</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Validation rate:       100.0%</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">‚úÖ All samples passed validation!</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Sample statistics:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üìà Sample Statistics:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Total samples:     2,129</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Average length:    721 characters</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Min length:        340 characters</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Max length:        3,880 characters</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Total characters:  1,536,016</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">&gt;&gt;&gt; Step 4: Saving processed data...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Training data saved to: data/processed/train.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Validation data saved to: data/processed/val.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Test data saved to: data/processed/test.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Data statistics saved to: data/processed/data_stats.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Training data: data/processed/train.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Validation data: data/processed/val.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Test data: data/processed/test.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Statistics: data/processed/data_stats.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Training samples: 1705</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Validation samples: 212</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Test samples: 212</span><br></span></code></pre></div></div></div></div></details>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="2-train-the-model">2. Train the model<a href="#2-train-the-model" class="hash-link" aria-label="Direct link to 2. Train the model" title="Direct link to 2. Train the model">‚Äã</a></h2>
<p>Code can be found here: <a href="https://github.com/DidierRLopes/fine-tune-llm/blob/main/scripts/02_train_model.py" target="_blank" rel="noopener noreferrer">https://github.com/DidierRLopes/fine-tune-llm/blob/main/scripts/02_train_model.py</a>.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="model-configuration">Model configuration<a href="#model-configuration" class="hash-link" aria-label="Direct link to Model configuration" title="Direct link to Model configuration">‚Äã</a></h3>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">base_model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">path</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;microsoft/Phi-3-mini-4k-instruct&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">lora</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">num_layers</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token number">32</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">lora_layers</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token number">32</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">rank</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token number">16</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">scale</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token number">20.0</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">dropout</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token number">0.1</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">keys</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;self_attn.q_proj&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;self_attn.k_proj&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;self_attn.v_proj&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;self_attn.o_proj&quot;</span><br></span></code></pre></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="lora-layers">Lora layers<a href="#lora-layers" class="hash-link" aria-label="Direct link to Lora layers" title="Direct link to Lora layers">‚Äã</a></h4>
<p>From <a href="https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/config.json" target="_blank" rel="noopener noreferrer">phi-3-mini-4k-instruct config file</a> we know that it has 32 hidden layers (i.e. 32 transformer blocks comprised of multi-head self-attention, feed-forward network, and residual connections + layer norms).</p>
<p>In transformer models like Phi-3, different layers learn different levels of abstraction:</p>
<ul>
<li><strong>Bottom layers (1-10) - &quot;grammar&quot;</strong>: Learn fundamental language patterns - grammar, syntax, basic word associations. These layers understand that &quot;CEO&quot; is a noun, that it often precedes &quot;of&quot;, and basic sentence structure.</li>
<li><strong>Middle layers (11-20) - &quot;reasoning&quot;</strong>: Build conceptual understanding - connecting ideas, understanding context, domain knowledge. These layers learn that &quot;OpenBB&quot; relates to &quot;finance&quot; and &quot;open source&quot;, that &quot;Terminal&quot; in my context means a financial analysis tool, not a computer interface.</li>
<li><strong>Top layers (21-32) - &quot;style &amp; expression&quot;</strong>: Handle vocabulary selection and style - choosing specific words, maintaining tone, formatting responses. These layers decide whether to say &quot;leverage&quot; vs &quot;use&quot;, whether to include technical details, how to structure explanations.</li>
</ul>
<p>Most fine-tuning tutorials suggest only adapting the top 8-16 layers because it&#x27;s more memory-efficient and often sufficient for simple style transfer. But my writing isn&#x27;t just about word choice - it&#x27;s about how I conceptually approach topics:</p>
<ul>
<li>When I discuss technical topics, I ground them in practical examples (middle layers)</li>
<li>I have specific patterns of explanation - starting broad, then diving deep (bottom-middle layers)</li>
<li>My sentence construction tends toward clarity over complexity (bottom layers)</li>
</ul>
<p>Also, I often change the order of the words due to Portuguese being my primary language - although not correct, ultimately I‚Äôm trying to fine-tune a model to represent my writing better. So, in this case, I adapted all 32 layers.</p>
<p>Although this happened when I was fine-tuning those 3.1M parameters üò≠</p>
<p align="center"><img width="600" src="/blog/
2025-09-02-fine-tuning-a-llm-on-my-blog-posts_5.png"></p>
<hr>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="attention-matrices">Attention matrices<a href="#attention-matrices" class="hash-link" aria-label="Direct link to Attention matrices" title="Direct link to Attention matrices">‚Äã</a></h4>
<p>In LoRA, the matrices you can adapt are:</p>
<ul>
<li><strong>Attention projections</strong>: Query, Key, Value, Output<!-- -->
<ul>
<li>This is the most common and impactful, it changes which tokens pay attention to which and how strongly. Ultimately, this is where the reasoning comes from.</li>
</ul>
</li>
<li><strong>Feed-forward (MLP) projections</strong>: W1, W2<!-- -->
<ul>
<li>Changes how representations are transformed nonlinearly - not the most common</li>
</ul>
</li>
<li><strong>Embeddings &amp; LM head</strong>: token embedding matrix and output head<!-- -->
<ul>
<li>Almost never adapted</li>
</ul>
</li>
</ul>
<p>We went with the attention projections to adapt the complete attention mechanism. Picking FFN matrices would make adapters 2-3x larger, and I didn‚Äôt think it was worth it.</p>
<hr>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="rank">Rank<a href="#rank" class="hash-link" aria-label="Direct link to Rank" title="Direct link to Rank">‚Äã</a></h4>
<p>LoRA works by adding to a big weight matrix of dimension <code>d x d</code>, and adapted weight matrix of dimension <code>(d,d)</code> - where <code>d</code> is the hidden_size and can be found in model config, in our case, <code>3072</code>.</p>
<p>So wait, LoRA adds a matrix of size <code>3072 √ó 3072</code>?</p>
<p>Yes and no.</p>
<p>It does, but in a smart way - and this is where <strong>rank</strong> comes into play.</p>
<p>LoRA adapts two matrices called <code>A</code> and <code>B</code>, where <code>A</code> is of dimension <code>(d,r)</code> and <code>B</code> is of dimension <code>(r,d)</code>. The <code>r</code> value is the <strong>rank</strong>. And although multiplying these matrices results in a matrix of size <code>(d,d)</code>, the number of parameters on <code>A</code> and <code>B</code> combined is <code>2dr</code>.</p>
<p>So,</p>
<ul>
<li><strong>Low rank (small r</strong>), LoRA can only make coarse adjustments (cheap, fast).</li>
<li><strong>High rank (large r)</strong>, LoRA can make finer adjustments (better fidelity, but heavier).</li>
</ul>
<p>I actually started with 8 on this one, but results weren‚Äôt the best so I doubled it.</p>
<p align="center"><img width="600" src="/blog/
2025-09-02-fine-tuning-a-llm-on-my-blog-posts_6.png"></p>
<p>Note: The <code>AxB</code> multiplication that happens which results in the new <code>W</code> matrix adapter is normalized by <code>r</code>. This makes it so the update‚Äôs magnitude stays roughly stable regardless of rank, otherwise <code>r</code> would linearly increase variance of the update.</p>
<hr>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="scale">Scale<a href="#scale" class="hash-link" aria-label="Direct link to Scale" title="Direct link to Scale">‚Äã</a></h4>
<p>What if you actually wanted there to be higher variance of the update?</p>
<p>As in, you wanted LoRA update to influence even more the frozen weight?</p>
<p>This is where the <strong>scale</strong> parameter <code>Œ±</code> comes into play.</p>
<ul>
<li><strong>Small scale Œ±</strong>: LoRA update has a subtle effect.</li>
<li><strong>Large scale Œ±</strong>: LoRA update dominates more strongly.</li>
</ul>
<p>Most PyTorch examples use <code>1.0</code>-<code>5.0</code>, but MLX&#x27;s implementation benefits from higher scales - the community suggests <code>2 * rank</code> as a rule of thumb, so that LoRA adapters weights don‚Äôt get drowned out by the base model frozen weights.</p>
<hr>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="dropout">Dropout<a href="#dropout" class="hash-link" aria-label="Direct link to Dropout" title="Direct link to Dropout">‚Äã</a></h4>
<p>Dropout <code>p</code> is cheap insurance against overfitting when your fine-tune dataset is narrow (e.g., aligning a model to one person‚Äôs writing style) or you have a small dataset (e.g. a few thousand samples). It works by randomly zeroing out parts of the low-rank update with probability <code>p</code> (0-1).</p>
<ul>
<li><strong>Too much dropout</strong>: Slows learning (adapter doesn‚Äôt specialize enough).</li>
<li><strong>Too little dropout</strong>: Adapter memorizes quirks instead of general style.</li>
</ul>
<p>If dataset is huge (not our case lol), often the dropout is skipped altogether since there isn‚Äôt the risk of overfitting (I mean it depends on the model a bit, but yea).</p>
<p>We went with <code>0.1</code>, which falls under the recommendation.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="training-configuration">Training Configuration<a href="#training-configuration" class="hash-link" aria-label="Direct link to Training Configuration" title="Direct link to Training Configuration">‚Äã</a></h3>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token key atrule">training</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">iters</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token number">2000</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">batch_size</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token number">4</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">learning_rate</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token number">1e-5</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">steps_per_eval</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token number">50</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">grad_checkpoint</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token boolean important">true</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token key atrule">optimizer</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">type</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;adam&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token key atrule">metrics</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">patience</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token number">5</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">min_delta</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token number">0.001</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token key atrule">paths</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">train_data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;data/processed/train.json&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">test_data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;data/processed/test.json&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">logs_dir</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;logs/training&quot;</span><br></span></code></pre></div></div>
<p>The training hyperparameters rationale:</p>
<ul>
<li>
<p><strong>Batch size 4</strong>: Larger batches (8-16) provide more stable gradients but require more memory and can miss fine details. Smaller batches (1-2) are noisy. 4 felt like a good sweet spot.</p>
<ul>
<li>With ~1,700 training samples and batch size 4, that means that there are 425 steps/epoch. An epoch corresponds to a full pass through the training dataset.</li>
</ul>
</li>
<li>
<p><strong>2000 iterations</strong>: With 425 steps per epoch and 2000 total steps, that means that there are roughly 5 epochs.</p>
<ul>
<li>This means that the model has seen each of the 1,700 examples ~5 times (in slightly different shuffles).</li>
<li>If you kept training further (10‚Äì20 epochs), you risk overfitting, memorizing samples instead of generalizing.</li>
</ul>
</li>
<li>
<p><strong>Adam</strong> stands for <strong>Adaptive Momentum Estimation</strong>, it is one of the model widely used optimizers in deep learning, and an extension of stochastic gradient descent (SGD). This was an easy choice.</p>
<ul>
<li>Adaptive - each parameter gets its own learning rate (scaled by gradient history).</li>
<li>Momentum - smooths updates, prevents oscillations.</li>
<li>Automatic scaling - no need to tune learning rate schedules as much as vanilla SGD.</li>
</ul>
</li>
<li>
<p><strong>Learning rate 1e-5</strong>: This controls how big each update step is when adjusting parameters during training. Luckily, there are good rules of thumbs for this value in LoRA / LLM fine-tuning:</p>
<ul>
<li>Full fine-tunes (big models): 1e-5 to 1e-4</li>
<li>LoRA fine-tunes (small adapters): 1e-5 is a very common sweet spot</li>
<li>If unstable: drop to 5e-6 or even 1e-6</li>
<li>If underfitting (loss barely moving): try 2e-5 or 3e-5</li>
</ul>
</li>
<li>
<p><strong>Evaluation every 50 steps</strong>: Frequent enough to catch overfitting early (40 checks across training), but not so frequent that it slows training.</p>
<ul>
<li>Each evaluation on 210 validation samples takes ~30 seconds.</li>
</ul>
</li>
<li>
<p><strong>Gradient checkpointing</strong> is a memory-saving technique that reduces GPU usage during training by only storing a subset of intermediate activations in the forward pass. When the backward pass runs, the missing activations are recomputed on the fly, which increases compute time but dramatically lowers memory requirements.</p>
<ul>
<li>Frees up memory so we can fit a batch size of 4 instead of 2, improving gradient stability.</li>
<li>The ~20% slower training is a reasonable trade-off compared to the benefits.</li>
</ul>
</li>
<li>
<p><strong>Early stopping (patience 5 and min delta 0.001)</strong>: If validation loss doesn&#x27;t improve by 0.001 for 5 consecutive evaluations (250 iterations), training halts.</p>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="the-training-process">The training process<a href="#the-training-process" class="hash-link" aria-label="Direct link to The training process" title="Direct link to The training process">‚Äã</a></h3>
<p align="center"><img width="1000" src="/blog/
2025-09-02-fine-tuning-a-llm-on-my-blog-posts_7.png"></p>
<p>Training isn&#x27;t just about pressing &quot;run&quot; and waiting. It&#x27;s an active process of monitoring, adjusting, and sometimes killing runs that aren&#x27;t working.</p>
<p>Let me walk you through what&#x27;s actually happening when we fine-tune the model:</p>
<ol>
<li>Load base Phi-3 model (3.8B parameters)</li>
<li>Freeze base model weights - these never change</li>
<li>Add LoRA adapters (3.1M trainable params, ~0.08% of total)</li>
<li>Train only the adapters using validation for monitoring</li>
</ol>
<p>The magic here is that the training only updates the tiny LoRA matrices, but the combined output leverages the full 3.8B parameter knowledge base.</p>
<p>During each forward pass, the frozen base model produces its output, and the LoRA adapters add their learned adjustments on top - scaled by a factor of 20.0 to make the adaptations significant enough to matter.</p>
<p>In fact, every forward pass, the model processes 4 samples (batch_size), predicts the next tokens, and compares them to the actual tokens. The loss quantifies how wrong it was. Backpropagation then updates only the LoRA weights to reduce this loss.</p>
<p>During backpropagation, gradients flow only to the LoRA adapters - the base model parameters never receive gradients and never change.</p>
<p>Every 50 steps, I evaluate on validation data.</p>
<ul>
<li>Training loss dropping, validation loss dropping<!-- -->
<ul>
<li><strong>Learning</strong> - this is what you want! Model is learning from training data and generalizing to unseen validation data.</li>
</ul>
</li>
<li>Training loss dropping, validation loss flat<!-- -->
<ul>
<li><strong>Approaching capacity</strong> - model is learning from the training data, but it‚Äôs no longer generalizing better to unseen data. The model is &quot;full&quot; with the representational flexibility given. This is different from underfitting, where the model wouldn‚Äôt even be learning more from the training data.</li>
</ul>
</li>
<li>Training loss dropping, validation loss rising<!-- -->
<ul>
<li><strong>Overfitting</strong> - the model is ‚Äúmemorizing‚Äù training data examples. Time to stop, or early stoppage will take care of it.</li>
</ul>
</li>
<li>Both losses flat<!-- -->
<ul>
<li>Learning rate too low or model saturated (and we need to go back to tweaking hyperparameters).</li>
</ul>
</li>
</ul>
<p>My training showed steady improvement until iteration ~1,500, where validation loss plateaued around 2.8. The training got halted here due to my patience-based early stoppage (patience 5 - i.e. 250 iterations, min delta 0.001). This saved me from overfitting and wasted compute on iterations 1,500-2,000 where no meaningful learning occurred.</p>
<p>Ultimately, this told me the model had extracted what it could from the data.</p>
<p>But what does a validation loss of 2.8 mean?</p>
<p>Validation loss of 2.8 translates to a perplexity of ~16.4. This means on average, the model thinks there are about 16 equally likely next tokens at each step.</p>
<p>For context, random guessing would be the vocabulary size (e.g. ~32,000) and a perfect model would have 1. So 16.4 is pretty good.</p>
<p>Note: The 3.1M parameters aren&#x27;t learning language from scratch - they&#x27;re learning how to nudge an already-capable model toward our specific use case. That&#x27;s why LoRA is so sample-efficient and why we can achieve good results with relatively small datasets.</p>
<p>The base model retains all its general knowledge while the LoRA adapters inject domain-specific expertise. It&#x27;s like having a general practitioner doctor (base model) who takes a specialized course (LoRA training) to better handle specific types of cases, without forgetting their general medical knowledge.</p>
<p>Training is fundamentally about finding the right balance: enough learning to improve performance, but not so much that you overfit to your training data. The validation curve is your compass, and early stopping is your safety net.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Training model logs</summary><div><div class="collapsibleContent_i85q"><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">$ python scripts/02_train_model.py --model-config config/model_config.yaml --training-config config/training_config.yaml --train-data data/processed/train.json --val-data data/processed/val.json --test-data data/processed/test.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">MODEL TRAINING PIPELINE</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Model config: config/model_config.yaml</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Training config: config/training_config.yaml</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Training data: data/processed/train.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Validation data: data/processed/val.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Test data: data/processed/test.json (for final evaluation only)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Metrics logging to: logs/training</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">STARTING FINE-TUNING PIPELINE</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loading training data from: data/processed/train.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loading validation data from: data/processed/val.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loaded 1705 training samples</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loaded 212 validation samples</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loading base model: microsoft/Phi-3-mini-4k-instruct</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Fetching 13 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:00&lt;00:00, 46924.23it/s]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Model loaded successfully</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">LoRA config saved to: models/adapters/adapter_config.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Freezing base model parameters...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Converting linear layers to LoRA layers...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">LoRA setup completed:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Trainable parameters: 3,145,728 (0.08%)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Total parameters:     3,824,225,280</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Setting up adam optimizer with learning rate: 1e-05</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">TRAINING CONFIGURATION SUMMARY</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üìä Dataset:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Training samples:   1705</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Validation samples: 212</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üîß LoRA Configuration:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Layers to adapt:  32/32</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">LoRA rank:        16</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">LoRA scale:       20.0</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Dropout:          0.1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Target layers:    self_attn.q_proj, self_attn.k_proj, self_attn.v_proj, self_attn.o_proj</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üìà Training Parameters:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Iterations:       2000</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Batch size:       4</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Learning rate:    1e-5</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Eval frequency:   50</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Grad checkpoint:  True</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Starting training...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Training on 1705 samples</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Validating on 212 samples</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Starting training..., iters: 2000</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Iter 1: Val loss 2.399, Val took 12.519s</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Iteration 1: Val loss = 2.3986, Perplexity = 11.01</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">‚Üí New best validation loss: 2.3986 (perplexity: 11.01)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Iter 10: Train loss 2.242, Learning Rate 1.000e-05, It/sec 0.805, Tokens/sec 484.578, Trained Tokens 6021, Peak mem 8.924 GB</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Iteration 10: Train loss = 2.2423</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Iter 20: Train loss 1.522, Learning Rate 1.000e-05, It/sec 0.815, Tokens/sec 487.475, Trained Tokens 12004, Peak mem 8.924 GB</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Iteration 20: Train loss = 1.5223</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Iter 30: Train loss 1.319, Learning Rate 1.000e-05, It/sec 0.580, Tokens/sec 464.444, Trained Tokens 20014, Peak mem 11.472 GB</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Iteration 30: Train loss = 1.3188</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">(...)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">TRAINING SUMMARY</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üìä Training Completion:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Total iterations:     200</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Early stopped:        True</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Final train loss:     0.9613</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Final val loss:       1.0499</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Best val loss:        1.0393</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Final perplexity:     2.86</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Best perplexity:      2.83</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">‚ö†Ô∏è  Training stopped early due to lack of improvement</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> Patience counter reached: 24/5</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üìÅ Output Files:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Adapters: models/adapters/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Training logs: logs/training/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Metrics plot: logs/training/training_metrics.png</span><br></span></code></pre></div></div></div></div></details>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="3-evaluate-the-model">3. Evaluate the model<a href="#3-evaluate-the-model" class="hash-link" aria-label="Direct link to 3. Evaluate the model" title="Direct link to 3. Evaluate the model">‚Äã</a></h2>
<p>Code can be found here: <a href="https://github.com/DidierRLopes/fine-tune-llm/blob/main/scripts/03_evaluate_model.py" target="_blank" rel="noopener noreferrer">https://github.com/DidierRLopes/fine-tune-llm/blob/main/scripts/03_evaluate_model.py</a>.</p>
<p>The evaluation process answers a critical question:</p>
<p><strong>Did the fine-tuning actually improve the model?</strong></p>
<p>We compare two models:</p>
<ol>
<li><strong>Base model</strong>: The original Phi-3-mini-4k-instruct (our baseline)</li>
<li><strong>Fine-tuned model</strong>: Base model + our trained LoRA adapters applied at runtime</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-1-loading-models-for-comparison">Step 1: Loading models for comparison<a href="#step-1-loading-models-for-comparison" class="hash-link" aria-label="Direct link to Step 1: Loading models for comparison" title="Direct link to Step 1: Loading models for comparison">‚Äã</a></h3>
<p>For the base model evaluation, you need need to load the original model from HF.</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> tokenizer </span><span class="token operator">=</span><span class="token plain"> load</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;microsoft/Phi-3-mini-4k-instruct&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre></div></div>
<p>For the fine-tuned model evaluation, you need to load the original model from HF AND apply the adapters at runtime.</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> tokenizer </span><span class="token operator">=</span><span class="token plain"> load</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;microsoft/Phi-3-mini-4k-instruct&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">freeze</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># Freeze the base weights</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">linear_to_lora_layers</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> lora_config</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># Add LoRA layers</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">load_weights</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;adapters.safetensors&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># Load trained weights</span><br></span></code></pre></div></div>
<p>We&#x27;re not loading a completely different model file. We&#x27;re taking the original model and applying the learned adapter weights on top of it at runtime.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-2-test-data-generation">Step 2: Test data generation<a href="#step-2-test-data-generation" class="hash-link" aria-label="Direct link to Step 2: Test data generation" title="Direct link to Step 2: Test data generation">‚Äã</a></h3>
<p>For each test question, both models generate responses.</p>
<p>E.g. &quot;What is machine learning?&quot;</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">base_response </span><span class="token operator">=</span><span class="token plain"> generate</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">base_model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> tokenizer</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> question</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> max_tokens</span><span class="token operator">=</span><span class="token number">200</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">finetuned_response </span><span class="token operator">=</span><span class="token plain"> generate</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">finetuned_model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> tokenizer</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> question</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> max_tokens</span><span class="token operator">=</span><span class="token number">200</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-3-measuring-performance">Step 3: Measuring performance<a href="#step-3-measuring-performance" class="hash-link" aria-label="Direct link to Step 3: Measuring performance" title="Direct link to Step 3: Measuring performance">‚Äã</a></h3>
<p>We utilized world overlap as a simple and interpretable metric. It compares predicted words vs reference answer words. Uses Jaccard similarity: overlap = intersection/union.</p>
<p>Example:</p>
<ul>
<li>Reference: &quot;Machine learning uses algorithms and data&quot;</li>
<li>Prediction: &quot;ML uses algorithms to learn from data&quot;</li>
<li>Overlap: 4 words match out of 7 unique = 57%</li>
</ul>
<p>A better eval (out-of-scope for this work) would be to implement a version of LMArena where I would ask a question, and get two answers (one from base model and one from fine-tuned) and then I would select which one looked more like my writing without knowing which is which.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Evaluate model logs</summary><div><div class="collapsibleContent_i85q"><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">$ python scripts/03_evaluate_model.py --config config/evaluation_config.yaml --test-data data/processed/test.json --adapters-path models/adapters --base-model microsoft/Phi-3-mini-4k-instruct</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">MODEL EVALUATION PIPELINE</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Standard evaluation: Base + Runtime LoRA Adapters</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üî∏ Step 1: Evaluating Base Model</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">EVALUATING MODEL: base_model</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loading model: microsoft/Phi-3-mini-4k-instruct</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Fetching 13 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:00&lt;00:00, 16008.79it/s]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Model loaded successfully</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loading test data from: data/processed/test.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Extracted 212 test questions</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Generating predictions for 212 questions...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Generating predictions:   0%|  </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">(...)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Generating predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 212/212 [15:24&lt;00:00,  4.36s/it]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Calculating metrics for 212 samples...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Calculating word overlap scores...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">BASE_MODEL EVALUATION RESULTS</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üìä Word Overlap Metrics:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Mean:    0.1574</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Median:  0.1586</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Std:     0.0443</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Range:   [0.0357, 0.2778]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üìè Length Statistics:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Predictions: 864.6 chars (¬±163.1)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">References:  405.8 chars (¬±186.8)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Metrics saved to: logs/evaluation/base_model_evaluation.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Evaluation completed! Results saved to: logs/evaluation/base_model_evaluation.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üî∏ Step 2: Evaluating Base Model + Runtime LoRA Adapters</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">EVALUATING MODEL: lora_runtime (Base + LoRA Adapters)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loading base model: microsoft/Phi-3-mini-4k-instruct</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Fetching 13 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:00&lt;00:00, 3873.96it/s]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loading adapter config: models/adapters/adapter_config.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Freezing base model parameters...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Applying LoRA adapters...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loading adapter weights: models/adapters/adapters.safetensors</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">‚úÖ Model with LoRA adapters loaded successfully</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Trainable parameters: 3,145,728 (0.08%)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Total parameters:     3,824,225,280</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loading test data from: data/processed/test.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Extracted 212 test questions</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Generating predictions for 212 questions...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Generating predictions:   0%|      </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">(...)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Generating predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 212/212 [07:19&lt;00:00,  2.07s/it]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Calculating metrics for 212 samples...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Calculating word overlap scores...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">LORA_RUNTIME EVALUATION RESULTS</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üìä Word Overlap Metrics:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Mean:    0.2008</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Median:  0.2025</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Std:     0.0591</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Range:   [0.0449, 0.3913]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üìè Length Statistics:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Predictions: 381.9 chars (¬±72.9)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">References:  405.8 chars (¬±186.8)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Metrics saved to: logs/evaluation/lora_runtime_evaluation.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Evaluation completed! Results saved to: logs/evaluation/lora_runtime_evaluation.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üî∏ Step 3: Model Comparison</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">MODEL COMPARISON</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üìä Model Performance Comparison:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">------------------------------------------------------------</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Rank   Model                     Score           Metric         </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">------------------------------------------------------------</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">1      lora_runtime              0.2008        Word Overlap</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">2      base_model                0.1574        Word Overlap</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">------------------------------------------------------------</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üéØ Best Model (lora_runtime) vs Baseline (base_model):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Score Improvement: +27.6%</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">base_model: 0.1574</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">lora_runtime: 0.2008</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Score comparison plot saved to: logs/evaluation/word_overlap_comparison.png</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Score distribution plot saved to: logs/evaluation/word_overlap_distributions.png</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">2025-08-18 01:11:15.187 python3[94401:196290991] The class &#x27;NSSavePanel&#x27; overrides the method identifier.  This method is implemented by class &#x27;NSWindow&#x27;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Comparison report saved to: logs/evaluation/model_comparison_report.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üìä Detailed comparison plots saved to: logs/evaluation/model_comparison_report.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">EVALUATION PIPELINE COMPLETED!</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üìÅ Output files:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Evaluation results: logs/evaluation/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Comparison plots: logs/evaluation/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üéâ Evaluation completed! Check the results in the logs directory.</span><br></span></code></pre></div></div></div></div></details>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="results-and-statistical-analysis">Results and statistical analysis<a href="#results-and-statistical-analysis" class="hash-link" aria-label="Direct link to Results and statistical analysis" title="Direct link to Results and statistical analysis">‚Äã</a></h2>
<p align="center"><img width="800" src="/blog/
2025-09-02-fine-tuning-a-llm-on-my-blog-posts_8.png"></p>
<p>The evaluation results tell a compelling story about the effectiveness of our LoRA fine-tuning approach.</p>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="performance-improvement">Performance improvement<a href="#performance-improvement" class="hash-link" aria-label="Direct link to Performance improvement" title="Direct link to Performance improvement">‚Äã</a></h3>
<ul>
<li><strong>Base model</strong>: 0.1574 word overlap (15.74%)</li>
<li><strong>FT model</strong>: 0.2008 word overlap (20.08%)</li>
</ul>
<p>Improvement: +27.6% better performance</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="consistency-analysis">Consistency analysis<a href="#consistency-analysis" class="hash-link" aria-label="Direct link to Consistency analysis" title="Direct link to Consistency analysis">‚Äã</a></h3>
<ul>
<li><strong>Base model std dev</strong>: 0.0443 (relatively consistent but limited)</li>
<li><strong>FT std dev</strong>: 0.0591 (slightly more variable, but higher overall performance)</li>
</ul>
<p>The increased standard deviation in the fine-tuned model actually tells a positive story. While the base model gives consistently mediocre responses, our fine-tuned model shows more range - it&#x27;s capable of both the baseline performance and significantly better responses.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="range-expansion">Range expansion<a href="#range-expansion" class="hash-link" aria-label="Direct link to Range expansion" title="Direct link to Range expansion">‚Äã</a></h3>
<ul>
<li><strong>Base model range</strong>: [0.0357, 0.2778]</li>
<li><strong>FT range</strong>: [0.0449, 0.3913]</li>
</ul>
<p>The fine-tuned model&#x27;s maximum score (0.3913) significantly exceeds the base model&#x27;s best performance (0.2778). This 41% jump in peak performance shows the model learned to generate responses that better match the reference style and content.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="length-analysis">Length analysis<a href="#length-analysis" class="hash-link" aria-label="Direct link to Length analysis" title="Direct link to Length analysis">‚Äã</a></h3>
<ul>
<li><strong>Reference Length</strong>: 405.8 characters (¬±186.8)</li>
<li><strong>Base model</strong>: 864.6 characters (¬±163.1) - verbose, unfocused</li>
<li><strong>FT model</strong>: 381.9 characters (¬±72.9) - concise, targeted</li>
</ul>
<p>This is perhaps the most telling insight. The base model generates responses that are 2.2x longer than the references, suggesting verbose, unfocused answers.</p>
<p>The fine-tuned model produces responses much closer to the reference length (381.9 vs 405.8 characters), with lower variability (¬±72.9 vs ¬±163.1), indicating it learned the appropriate response style and length.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="parameter-efficiency">Parameter efficiency<a href="#parameter-efficiency" class="hash-link" aria-label="Direct link to Parameter efficiency" title="Direct link to Parameter efficiency">‚Äã</a></h3>
<ul>
<li>Total parameters: 3.82B</li>
<li>Trainable parameters: 3.15M (0.08%)</li>
<li>Performance gain: +27.6%</li>
</ul>
<p>We achieved nearly 28% better performance by training just 0.08% of the model&#x27;s parameters. This represents exceptional parameter efficiency - roughly 345% improvement per percentage point of parameters trained.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="statistical-interpretation">Statistical interpretation<a href="#statistical-interpretation" class="hash-link" aria-label="Direct link to Statistical interpretation" title="Direct link to Statistical interpretation">‚Äã</a></h3>
<p>What 20.08% Word Overlap Actually Means: In a 100-word response, our fine-tuned model shares about 20 words with the reference answer. While this might seem low, it&#x27;s actually strong performance because:</p>
<ul>
<li><strong>Multiple valid phrasing</strong>: &quot;quick&quot; vs &quot;fast&quot;, &quot;big&quot; vs &quot;large&quot;</li>
<li><strong>Different valid approaches</strong>: Same facts, different explanations</li>
<li><strong>Context variations</strong>: References are specific examples, predictions are generalizations</li>
</ul>
<p>The key insight is relative improvement. The base model achieved 15.74% overlap - our fine-tuning pushed this to 20.08%, crossing an important threshold where responses become noticeably more aligned with expected answers.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="vibe-checking">Vibe checking<a href="#vibe-checking" class="hash-link" aria-label="Direct link to Vibe checking" title="Direct link to Vibe checking">‚Äã</a></h2>
<p>Results and statistics are great, but there‚Äôs nothing like actually playing with the models and vibe check results. This is why I created <a href="https://github.com/DidierRLopes/fine-tune-llm/blob/main/scripts/interactive_chat.py" target="_blank" rel="noopener noreferrer">interactive_chat.py file</a>, so that we can ask a question - and we get responses from the base model but also fine-tuned model (and fused if it exists).</p>
<p><em><strong>Prompt: &quot;What is machine learning?&quot;</strong></em></p>
<p><strong>Base model</strong>: <em>&quot;Machine learning is a subset of artificial intelligence (AI) that focuses on the development of computer programs and algorithms that can access data and use it to learn for themselves. In machine learning, algorithms use computational methods to identify patterns in data, learn from those patterns, and then make decisions or predictions based on the data they&#x27;ve studied.</em></p>
<p><em>There are three main types of machine learning: supervised learning, unsupervised learning, and reinforcement learning.</em></p>
<p><em>1. Supervised learning: In this approach, the algorithm is trained on a labeled dataset, meaning that the input data is paired with the desired output. The algorithm learns to map input data to output data based on this training. The goal is to create a model that can accurately predict the output for new, unseen data. Examples of supervised learning include classification (predicting a category or label) and regression (predicting a numerical value).</em></p>
<p><em>2. Unsupervised learning&quot;</em></p>
<p><strong>LoRA runtime (base + adapters)</strong>: <em>Machine learning is a subfield of artificial intelligence that focuses on the development of algorithms and statistical models to enable computers to learn and improve from data, without being explicitly programmed. It encompasses techniques such as supervised learning, unsupervised learning, and reinforcement learning, which enable systems to make predictions, detect patterns, and make decisions based on large amounts of data.</em></p>
<p><strong>Fused model</strong>: <em>Machine learning is a field of computer science that focuses on building algorithms capable of learning and improving from data, without explicit programming. It involves developing models that can make predictions or decisions based on input data, often through complex statistical techniques and mathematical optimization.</em></p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Interactive chat logs</summary><div><div class="collapsibleContent_i85q"><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">$ python scripts/interactive_chat.py</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">INTERACTIVE CHAT INTERFACE</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Mode: COMPARE</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üî∏ Loading base model...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Fetching 13 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:00&lt;00:00, 51198.08it/s]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">‚úÖ Base model loaded</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üî∏ Loading LoRA runtime (base + adapters)...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Fetching 13 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:00&lt;00:00, 38130.04it/s]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">‚úÖ LoRA runtime loaded</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üî∏ Loading fused model...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">‚úÖ Fused model loaded</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">‚úÖ Loaded 3 models: [&#x27;base&#x27;, &#x27;lora_runtime&#x27;, &#x27;fused&#x27;]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">INTERACTIVE COMPARISON MODE</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Type your prompts to compare model responses.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Commands: &#x27;quit&#x27; to exit, &#x27;clear&#x27; to clear screen</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">------------------------------------------------------------</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üë§ You: What is machine learning</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">ü§ñ Generating from base...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">ü§ñ Generating from lora_runtime...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">ü§ñ Generating from fused...</span><br></span></code></pre></div></div></div></div></details>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="model-on-hugging-face">Model on Hugging Face<a href="#model-on-hugging-face" class="hash-link" aria-label="Direct link to Model on Hugging Face" title="Direct link to Model on Hugging Face">‚Äã</a></h2>
<p>Finally, using the <a href="https://github.com/DidierRLopes/fine-tune-llm/blob/main/scripts/upload_model_to_hf.py" target="_blank" rel="noopener noreferrer">script upload_model_to_hf.py</a>, I have uploaded the model to <a href="https://huggingface.co/didierlopes/phi-3-mini-4k-instruct-ft-on-my-blog" target="_blank" rel="noopener noreferrer">my Hugging Face repo</a>.</p>
<p align="center"><img width="600" src="/blog/
2025-09-02-fine-tuning-a-llm-on-my-blog-posts_9.png"></p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Uploading model to hugging face logs</summary><div><div class="collapsibleContent_i85q"><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">$ python scripts/05_upload_model.py --repo-name didierlopes/phi-3-mini-4k-instruct-ft-on-my-blog</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">MODEL UPLOAD PIPELINE</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Model path: models/adapters</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Model type: LoRA Adapters</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Repository: didierlopes/phi-3-mini-4k-instruct-ft-on-my-blog</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Private: False</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Dry run: False</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Step 1: Validating model structure...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">‚ö†Ô∏è  Warning: Missing recommended files: [&#x27;config.json&#x27;]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">‚úÖ Model validation passed</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Found 21 model weight files</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Total model size: 252.1 MB</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Step 2: Initializing HuggingFace API...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">‚úÖ Authenticated as: didierlopes</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Step 3: Creating repository...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">‚úÖ Repository ready: https://huggingface.co/didierlopes/phi-3-mini-4k-instruct-ft-on-my-blog</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Step 4: Creating model card...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">‚úÖ Model card created: models/adapters/README.md</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Step 5: Uploading model files...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Uploading 23 files...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Processing Files (21 / 21)              : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|  264MB /  264MB,  441MB/s  </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">New Data Upload                         : |                                                                                                                                                                                                                      |  0.00B /  0.00B,  0.00B/s  </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">...apters/0000800_adapters.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12.6MB / 12.6MB            </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">...apters/0002000_adapters.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12.6MB / 12.6MB            </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">...apters/0000500_adapters.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12.6MB / 12.6MB            </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">...apters/0001900_adapters.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12.6MB / 12.6MB            </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">...apters/0001500_adapters.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12.6MB / 12.6MB            </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">...apters/0000600_adapters.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12.6MB / 12.6MB            </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">...apters/0000700_adapters.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12.6MB / 12.6MB            </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">...apters/0001000_adapters.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12.6MB / 12.6MB            </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">...apters/0001100_adapters.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12.6MB / 12.6MB            </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">...apters/0000900_adapters.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12.6MB / 12.6MB            </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">No files have been modified since last commit. Skipping to prevent empty commit.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">‚úÖ Upload completed successfully!</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Step 6: Verifying upload...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">‚úÖ Upload verified: 24 files in repository</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">MODEL UPLOAD COMPLETED SUCCESSFULLY!</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üéâ Model uploaded to: https://huggingface.co/didierlopes/phi-3-mini-4k-instruct-ft-on-my-blog</span><br></span></code></pre></div></div></div></div></details>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="fused-model">Fused model<a href="#fused-model" class="hash-link" aria-label="Direct link to Fused model" title="Direct link to Fused model">‚Äã</a></h2>
<p>Code can be found here: <a href="https://github.com/DidierRLopes/fine-tune-llm/blob/main/scripts/04_fuse_and_evaluate.py" target="_blank" rel="noopener noreferrer">https://github.com/DidierRLopes/fine-tune-llm/blob/main/scripts/04_fuse_and_evaluate.py</a>.</p>
<p>While LoRA adapters are excellent for experimentation and sharing, production deployments often benefit from a single, unified model file. This is where fusion comes in.</p>
<p align="center"><img width="400" src="/blog/
2025-09-02-fine-tuning-a-llm-on-my-blog-posts_10.png"></p>
<p>Fusion mathematically merges your LoRA adapter weights back into the base model. It is recommended to use the fused model when:</p>
<ul>
<li>Deploying to production</li>
<li>Inference speed is critical</li>
<li>You want a single model file</li>
<li>Sharing with users who aren&#x27;t familiar with LoRA</li>
</ul>
<p>It also uses less memory, since you have a single model instead of a model + its adapters.</p>
<p>However, keeping adapters may still be important, particularly when</p>
<ul>
<li>Experimenting with different configurations<!-- -->
<ul>
<li>E.g. if you wanted to fine tune based on your blogs</li>
</ul>
</li>
<li>Storage/bandwidth is limited<!-- -->
<ul>
<li>This is why I pushed the adapters to HF and not the fused model</li>
</ul>
</li>
<li>You need to swap between multiple fine-tuned versions</li>
</ul>
<p>Note: Fusion is a one-way operation. Once fused, you can&#x27;t extract the adapters back out. Always keep your original adapter files as your &quot;source of truth&quot;. Think of the fused model as a compiled binary, and your adapters as the source code.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Fused model logs</summary><div><div class="collapsibleContent_i85q"><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">$ python scripts/04_fuse_and_evaluate.py </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">FUSION AND COMPREHENSIVE EVALUATION PIPELINE</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">This script: 1) Fuses adapters  2) Evaluates Base + Runtime + Fused</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Base model: microsoft/Phi-3-mini-4k-instruct</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Adapters: models/adapters</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Fused output: models/fused</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Test data: data/processed/test.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">STEP 1: ADAPTER FUSION</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">‚úÖ Fusion inputs validated successfully</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">FUSING LORA ADAPTERS</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Base model: microsoft/Phi-3-mini-4k-instruct</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Adapters: models/adapters</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Output: models/fused</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Command: mlx_lm.fuse --model microsoft/Phi-3-mini-4k-instruct --adapter-path models/adapters --save-path models/fused</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">------------------------------------------------------------</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loading pretrained model</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Fetching 13 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:00&lt;00:00, 15029.20it/s]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">------------------------------------------------------------</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">‚úÖ Fusion completed successfully!</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Fused model saved to: models/fused</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">‚úÖ Fusion completed: models/fused</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">STEP 2: COMPREHENSIVE EVALUATION</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Evaluating: Base + Runtime LoRA + Fused</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">================================================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">COMPREHENSIVE MODEL COMPARISON</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Base Model vs Runtime LoRA vs Fused Model</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">================================================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üî∏ Step 1: Evaluating Base Model</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">EVALUATING MODEL: base_model</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loading model: microsoft/Phi-3-mini-4k-instruct</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Fetching 13 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:00&lt;00:00, 16881.10it/s]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Model loaded successfully</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loading test data from: data/processed/test.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Extracted 212 test questions</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Generating predictions for 212 questions...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Generating predictions:   0%| </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">(...)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Generating predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 212/212 [15:08&lt;00:00,  4.29s/it]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Calculating metrics for 212 samples...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Calculating word overlap scores...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">BASE_MODEL EVALUATION RESULTS</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üìä Word Overlap Metrics:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Mean:    0.1563</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Median:  0.1532</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Std:     0.0468</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Range:   [0.0000, 0.3333]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üìè Length Statistics:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Predictions: 866.8 chars (¬±154.2)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">References:  405.8 chars (¬±186.8)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Metrics saved to: logs/evaluation/base_model_evaluation.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Evaluation completed! Results saved to: logs/evaluation/base_model_evaluation.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üî∏ Step 2: Evaluating Base Model + Runtime LoRA Adapters</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">EVALUATING MODEL: lora_runtime (Base + LoRA Adapters)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loading base model: microsoft/Phi-3-mini-4k-instruct</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Fetching 13 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:00&lt;00:00, 13977.43it/s]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loading adapter config: models/adapters/adapter_config.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Freezing base model parameters...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Applying LoRA adapters...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loading adapter weights: models/adapters/adapters.safetensors</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">‚úÖ Model with LoRA adapters loaded successfully</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Trainable parameters: 3,145,728 (0.08%)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Total parameters:     3,824,225,280</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loading test data from: data/processed/test.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Extracted 212 test questions</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Generating predictions for 212 questions...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Generating predictions:   0%|             </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">(...)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Generating predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 212/212 [07:07&lt;00:00,  2.02s/it]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Calculating metrics for 212 samples...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Calculating word overlap scores...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">LORA_RUNTIME EVALUATION RESULTS</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üìä Word Overlap Metrics:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Mean:    0.1940</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Median:  0.1847</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Std:     0.0627</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Range:   [0.0658, 0.3846]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üìè Length Statistics:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Predictions: 377.1 chars (¬±70.4)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">References:  405.8 chars (¬±186.8)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Metrics saved to: logs/evaluation/lora_runtime_evaluation.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Evaluation completed! Results saved to: logs/evaluation/lora_runtime_evaluation.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üî∏ Step 3: Evaluating Fused Model</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">EVALUATING MODEL: lora_fused</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loading model: models/fused</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Model loaded successfully</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Loading test data from: data/processed/test.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Extracted 212 test questions</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Generating predictions for 212 questions...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Generating predictions:   0%|  </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">(...)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Generating predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 212/212 [06:23&lt;00:00,  1.81s/it]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Calculating metrics for 212 samples...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Calculating word overlap scores...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">LORA_FUSED EVALUATION RESULTS</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üìä Word Overlap Metrics:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Mean:    0.2027</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Median:  0.2000</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Std:     0.0638</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Range:   [0.0690, 0.4521]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üìè Length Statistics:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Predictions: 379.1 chars (¬±61.8)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">References:  405.8 chars (¬±186.8)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Metrics saved to: logs/evaluation/lora_fused_evaluation.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Evaluation completed! Results saved to: logs/evaluation/lora_fused_evaluation.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üî∏ Step 4: Model Comparison</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">MODEL COMPARISON</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üìä Model Performance Comparison:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">------------------------------------------------------------</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Rank   Model                     Score           Metric         </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">------------------------------------------------------------</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">1      lora_fused                0.2027        Word Overlap</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">2      lora_runtime              0.1940        Word Overlap</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">3      base_model                0.1563        Word Overlap</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">------------------------------------------------------------</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üéØ Best Model (lora_fused) vs Baseline (base_model):</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Score Improvement: +29.7%</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">base_model: 0.1563</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">lora_fused: 0.2027</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üî∏ Step 5: Fusion Verification</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">FUSION QUALITY VERIFICATION</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üìä Word Overlap Comparison:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Runtime Adapters: 0.1940</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Fused Model:      0.2027</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Absolute Diff:    0.0086</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Relative Diff:    4.45%</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">‚ö†Ô∏è  Acceptable fusion quality - some degradation detected</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üìà Additional Metrics Comparison:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Word Overlap Std Dev: Runtime 0.0627 | Fused 0.0638</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Word Overlap Range:   Runtime [0.066, 0.385] | Fused [0.069, 0.452]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üìÅ Comprehensive results saved to: logs/evaluation/comprehensive_comparison_20250818_021105.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">PIPELINE COMPLETED SUCCESSFULLY!</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">============================================================</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üìä Models Evaluated: base_model, lora_runtime, lora_fused</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üéØ Key Results:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">base_model: 0.1563 (Word Overlap)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">lora_runtime: 0.1940 (Word Overlap)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">lora_fused: 0.2027 (Word Overlap)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üìÅ Detailed results saved to: logs/evaluation/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">üìÅ Fused model available at: models/fused</span><br></span></code></pre></div></div></div></div></details>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="wrap-up">Wrap up<a href="#wrap-up" class="hash-link" aria-label="Direct link to Wrap up" title="Direct link to Wrap up">‚Äã</a></h2>
<p>And that‚Äôs it!</p>
<p>The results demonstrate that LoRA fine-tuning achieved its goal: meaningful performance improvement with minimal computational overhead, faster inference, and more focused responses that better match the target domain.</p>
<p>I hope you find this helpful, all the code can be found here: <a href="https://github.com/DidierRLopes/fine-tune-llm" target="_blank" rel="noopener noreferrer">https://github.com/DidierRLopes/fine-tune-llm</a>.</p></div><footer class="docusaurus-mt-lg"><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/blog/tags/ai">ai</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/blog/tags/machine-learning">machine-learning</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/blog/tags/open-source">open-source</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/blog/tags/tutorial">tutorial</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/blog/tags/apple-silicon">apple-silicon</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/blog/tags/llm">llm</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/blog/tags/fine-tuning">fine-tuning</a></li></ul></div></div><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><a href="https://github.com/DidierRLopes/my-website/tree/main/blog/2025-09-02-fine-tuning-a-llm-on-my-blog-posts.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article></div><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/blog/2025-09-06-tracking-your-partner-health-stats-with-oura-and-ai"><div class="pagination-nav__sublabel">Newer post</div><div class="pagination-nav__label">Tracking your partner&#x27;s health stats with Oura and AI</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/blog/2025-08-25-top-10-openbb-apps"><div class="pagination-nav__sublabel">Older post</div><div class="pagination-nav__label">Top 10 OpenBB apps</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#context" class="table-of-contents__link toc-highlight">Context</a></li><li><a href="#0-setting-up-the-foundation" class="table-of-contents__link toc-highlight">0. Setting up the foundation</a><ul><li><a href="#model" class="table-of-contents__link toc-highlight">Model</a></li><li><a href="#finetuning-technique" class="table-of-contents__link toc-highlight">Finetuning Technique</a></li><li><a href="#framework" class="table-of-contents__link toc-highlight">Framework</a></li></ul></li><li><a href="#1-preparing-the-data" class="table-of-contents__link toc-highlight">1. Preparing the data</a><ul><li><a href="#formatting-for-phi-3-mini-4k-instruct" class="table-of-contents__link toc-highlight">Formatting for phi-3-mini-4k-instruct</a></li><li><a href="#training-split" class="table-of-contents__link toc-highlight">Training split</a></li></ul></li><li><a href="#2-train-the-model" class="table-of-contents__link toc-highlight">2. Train the model</a><ul><li><a href="#model-configuration" class="table-of-contents__link toc-highlight">Model configuration</a></li><li><a href="#training-configuration" class="table-of-contents__link toc-highlight">Training Configuration</a></li><li><a href="#the-training-process" class="table-of-contents__link toc-highlight">The training process</a></li></ul></li><li><a href="#3-evaluate-the-model" class="table-of-contents__link toc-highlight">3. Evaluate the model</a><ul><li><a href="#step-1-loading-models-for-comparison" class="table-of-contents__link toc-highlight">Step 1: Loading models for comparison</a></li><li><a href="#step-2-test-data-generation" class="table-of-contents__link toc-highlight">Step 2: Test data generation</a></li><li><a href="#step-3-measuring-performance" class="table-of-contents__link toc-highlight">Step 3: Measuring performance</a></li></ul></li><li><a href="#results-and-statistical-analysis" class="table-of-contents__link toc-highlight">Results and statistical analysis</a><ul><li><a href="#performance-improvement" class="table-of-contents__link toc-highlight">Performance improvement</a></li><li><a href="#consistency-analysis" class="table-of-contents__link toc-highlight">Consistency analysis</a></li><li><a href="#range-expansion" class="table-of-contents__link toc-highlight">Range expansion</a></li><li><a href="#length-analysis" class="table-of-contents__link toc-highlight">Length analysis</a></li><li><a href="#parameter-efficiency" class="table-of-contents__link toc-highlight">Parameter efficiency</a></li><li><a href="#statistical-interpretation" class="table-of-contents__link toc-highlight">Statistical interpretation</a></li></ul></li><li><a href="#vibe-checking" class="table-of-contents__link toc-highlight">Vibe checking</a></li><li><a href="#model-on-hugging-face" class="table-of-contents__link toc-highlight">Model on Hugging Face</a></li><li><a href="#fused-model" class="table-of-contents__link toc-highlight">Fused model</a></li><li><a href="#wrap-up" class="table-of-contents__link toc-highlight">Wrap up</a></li></ul></div></div></div></div></div><div class="mt-4"><footer class="bg-[#f8f9fa] dark:bg-[#000] text-[var(--ifm-footer-color)] pb-8"><div class="container container-fluid"><div class="row flex justify-center items-center"><div class="col text-center"><h4 class="mb-4">Let&#x27;s stay in touch.</h4><div class="flex justify-center space-x-4"><a href="https://twitter.com/didier_lopes" target="_blank" rel="noreferrer" class="text-black dark:text-white hover:text-ds-blue-primary dark:hover:text-ds-blue-accent"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" aria-label="Twitter"><title>Twitter</title><path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"></path></svg></a><a href="https://www.linkedin.com/in/didier-lopes/" target="_blank" rel="noreferrer" class="text-black dark:text-white hover:text-ds-blue-primary dark:hover:text-ds-blue-accent"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" aria-label="LinkedIn"><title>LinkedIn</title><path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z"></path></svg></a><a href="https://github.com/DidierRLopes" target="_blank" rel="noreferrer" class="text-[#333] dark:text-[#f5f5f5] hover:text-ds-blue-primary dark:hover:text-ds-blue-accent"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" aria-label="GitHub"><title>GitHub</title><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg></a><a href="https://cal.com/didierlopes/15min" target="_blank" rel="noreferrer" class="text-black dark:text-white hover:text-ds-blue-primary dark:hover:text-ds-blue-accent" aria-label="Book a meeting"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" aria-label="Calendar"><title>Cal.com</title><path d="M19 4h-1V2h-2v2H8V2H6v2H5c-1.11 0-1.99.9-1.99 2L3 20c0 1.1.89 2 2 2h14c1.1 0 2-.9 2-2V6c0-1.1-.9-2-2-2zm0 16H5V10h14v10zM9 14H7v-2h2v2zm4 0h-2v-2h2v2zm4 0h-2v-2h2v2zm-8 4H7v-2h2v2zm4 0h-2v-2h2v2zm4 0h-2v-2h2v2z"></path></svg></a></div></div></div></div></footer></div></div>
</body>
</html>