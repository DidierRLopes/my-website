<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">ChatGPT and The Future of AI in Finance | Didier Lopes</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://didierlopes.com/blog/chatgpt-and-the-future-of-ai-in-finance"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="ChatGPT and The Future of AI in Finance | Didier Lopes"><meta data-rh="true" name="description" content="I took the stage at the Cornell Quant Conference alongside Yu Yu (BlackRock) Tony Berkman (Two Sigma), and Samson Qian (Citadel), to discuss ChatGPT &amp; The Future of AI in Finance."><meta data-rh="true" property="og:description" content="I took the stage at the Cornell Quant Conference alongside Yu Yu (BlackRock) Tony Berkman (Two Sigma), and Samson Qian (Citadel), to discuss ChatGPT &amp; The Future of AI in Finance."><meta data-rh="true" property="og:image" content="https://didierlopes.com/blog/2024-09-21-chatgpt-and-the-future-of-ai-in-finance.jpg"><meta data-rh="true" name="twitter:image" content="https://didierlopes.com/blog/2024-09-21-chatgpt-and-the-future-of-ai-in-finance.jpg"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2024-09-21T00:00:00.000Z"><meta data-rh="true" property="article:tag" content="ai,fintech,llm,agents,hallucinations,fine-tuning,data-security,openbb,thought-leadership"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://didierlopes.com/blog/chatgpt-and-the-future-of-ai-in-finance"><link data-rh="true" rel="alternate" href="https://didierlopes.com/blog/chatgpt-and-the-future-of-ai-in-finance" hreflang="en"><link data-rh="true" rel="alternate" href="https://didierlopes.com/blog/chatgpt-and-the-future-of-ai-in-finance" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://CTGM87XQE8-dsn.algolia.net" crossorigin="anonymous"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","@id":"https://didierlopes.com/blog/chatgpt-and-the-future-of-ai-in-finance","mainEntityOfPage":"https://didierlopes.com/blog/chatgpt-and-the-future-of-ai-in-finance","url":"https://didierlopes.com/blog/chatgpt-and-the-future-of-ai-in-finance","headline":"ChatGPT and The Future of AI in Finance","name":"ChatGPT and The Future of AI in Finance","description":"I took the stage at the Cornell Quant Conference alongside Yu Yu (BlackRock) Tony Berkman (Two Sigma), and Samson Qian (Citadel), to discuss ChatGPT & The Future of AI in Finance.","datePublished":"2024-09-21T00:00:00.000Z","author":[],"image":{"@type":"ImageObject","@id":"https://didierlopes.com/blog/2024-09-21-chatgpt-and-the-future-of-ai-in-finance.jpg","url":"https://didierlopes.com/blog/2024-09-21-chatgpt-and-the-future-of-ai-in-finance.jpg","contentUrl":"https://didierlopes.com/blog/2024-09-21-chatgpt-and-the-future-of-ai-in-finance.jpg","caption":"title image for the blog post: ChatGPT and The Future of AI in Finance"},"keywords":[],"isPartOf":{"@type":"Blog","@id":"https://didierlopes.com/blog","name":"Blog"}}</script><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Didier Lopes RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Didier Lopes Atom Feed">
<link rel="alternate" type="application/json" href="/blog/feed.json" title="Didier Lopes JSON Feed">




<link rel="search" type="application/opensearchdescription+xml" title="Didier Lopes" href="/opensearch.xml">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script>window.dataLayer=window.dataLayer||[]</script>
<script>!function(e,t,a,n){e[n]=e[n]||[],e[n].push({"gtm.start":(new Date).getTime(),event:"gtm.js"});var g=t.getElementsByTagName(a)[0],m=t.createElement(a);m.async=!0,m.src="https://www.googletagmanager.com/gtm.js?id=GTM-PL77JR5L",g.parentNode.insertBefore(m,g)}(window,document,"script","dataLayer")</script>


<link rel="stylesheet" href="src/css/custom.css"><link rel="stylesheet" href="/assets/css/styles.558be8e4.css">
<script src="/assets/js/runtime~main.228a4098.js" defer="defer"></script>
<script src="/assets/js/main.28ba7790.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="dark";var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script>

<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PL77JR5L" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" target="_self" href="/"><div class="navbar__logo"><img src="/img/goku_pixel.png" alt="Didier Lopes logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/goku_pixel.png" alt="Didier Lopes logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Didier Lopes</b></a><div class="navbar__item"><div class="navbar__items--left-wrapper"></div><a class="navbar__item navbar__link" href="/books/to-read">Books</a><a class="navbar__item navbar__link" href="/media/on-stage">Media</a><a class="navbar__item navbar__link" href="/resume/experience">Resume</a><div class="navbar__item"></div></div><a aria-current="page" class="navbar__item navbar__link navbar__center-item navbar__link--active" href="/blog">Blog</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/intelligence">Intelligence</a><a class="navbar__item navbar__link" href="/chat">Chat</a><a href="/newsletter" class="navbar__item navbar__link navbar-item-external navbar-newsletter-link" target="_blank" rel="noopener noreferrer">Newsletter</a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><main class="col col--9 col--offset-1"><div><article class=""><header><h1 class="title_f1Hy">ChatGPT and The Future of AI in Finance</h1><div class="container_mt6G margin-vert--md"><time datetime="2024-09-21T00:00:00.000Z">September 21, 2024</time> Â· <!-- -->18 min read</div></header><div id="__blog-post-container" class="markdown"><p align="center"><img width="900" src="/blog/2024-09-21-chatgpt-and-the-future-of-ai-in-finance.jpg"></p>
<p>I took the stage at the Cornell Quant Conference alongside Yu Yu (BlackRock) Tony Berkman (Two Sigma), and Samson Qian (Citadel), to discuss ChatGPT &amp; The Future of AI in Finance.</p>
<div style="border-top:1px solid #0088CC;margin:1.5em 0"></div>
<p>Last week, I participated in a panel at the Cornell Financial Engineering Manhattan Conference. The topic of the panel was &#x27;ChatGPT &amp; The Future of AI in Finance.&#x27;</p>
<p>The other panelists were:</p>
<ul>
<li><strong>Yu Yu</strong>, Director of Data Science - BlackRock</li>
<li><strong>Tony Berkman</strong>, Managing Director - Two Sigma</li>
<li><strong>Samson Qian</strong>, Trader - Citadel</li>
</ul>
<p>After the discussion, several people reached out, mentioning it was one of their favorite panels of the day.</p>
<p>Since this wasn&#x27;t recorded, I took the opportunity to write down some of the topics discussed, along with a few additional thoughts that I believe in.</p>
<p>I will organize the following sections based on the topics discussed at the event:</p>
<ol>
<li>Hallucinations</li>
<li>Agents are the future</li>
<li>When does it make sense to fine-tune?</li>
<li>Compliance and Data security</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="1-hallucinations">1. Hallucinations<a href="#1-hallucinations" class="hash-link" aria-label="Direct link to 1. Hallucinations" title="Direct link to 1. Hallucinations">â€‹</a></h2>
<p>When talking about the topic of hallucinations, I have a <a href="https://x.com/didier_lopes/status/1675630822093918209" target="_blank" rel="noopener noreferrer">quote</a> that I love from Marc Andreesen:</p>
<blockquote>
<p>&quot;Hallucination is what we call when we don&#x27;t like it. Creativity is what we call it when we do like it.&quot;</p>
</blockquote>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="confident-hallucinations">Confident hallucinations<a href="#confident-hallucinations" class="hash-link" aria-label="Direct link to Confident hallucinations" title="Direct link to Confident hallucinations">â€‹</a></h3>
<p>The fundamental issue with hallucinations is the fact that the model hallucinates with confidence.</p>
<p>Imagine asking two different friends: &quot;Do you know where location X is?&quot;</p>
<p><strong>Friend A</strong>: It&#x27;s there.</p>
<p><strong>Friend B</strong>: Hmm, I&#x27;m not really sure. If I had to guess, I&#x27;d say there, but I&#x27;m not 100% certain.</p>
<p>If both gave wrong directions, you would consider <strong>Friend A</strong> a liar, but not Friend B. This is because <strong>Friend B</strong> lacked confidence in their answer, they were trying to help but highlighted that they weren&#x27;t sure about it.</p>
<p>The problem with current LLMs is that they are, for the most part, like <strong>Friend A</strong>. They say wrong things with certainty.</p>
<p>Hallucinations would be less problematic if the default behavior were more like the answer on the right, when the LLM is not 100% confident.</p>
<div class="flex justify-center items-center"><img width="350" src="/blog/2024-09-21-chatgpt-and-the-future-of-ai-in-finance_1.png" style="margin-right:10px"><img width="350" src="/blog/2024-09-21-chatgpt-and-the-future-of-ai-in-finance_2.png"></div>
<br>
<p>The problem with confident hallucinations is that, similar to why everyone dislikes liars, it leads to a lack of trust. So users begin to put everything that is output by an LLM under a microscope - even if what the model says is accurate.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-avoid-hallucinations">How to avoid hallucinations<a href="#how-to-avoid-hallucinations" class="hash-link" aria-label="Direct link to How to avoid hallucinations" title="Direct link to How to avoid hallucinations">â€‹</a></h3>
<p>There are ways to address this and one of the key approaches we are extremely strong about at OpenBB is always tapping into information that is available.</p>
<p>When a user asks a question that requires financial data, the OpenBB Copilot always searches for that data on OpenBB (either through data we make available or through private data that customers bring).</p>
<p>The Copilot will only answer the question if that data exists. This allows the model to cite the data used in its response, so the user can double-check.</p>
<p>This is how it looks.</p>
<p align="center"><img width="900" src="/blog/2024-09-21-chatgpt-and-the-future-of-ai-in-finance_3.png"></p>
<p>While I&#x27;ve heard a few vendors promising 100% accuracy, this is simply not true.</p>
<p>We are at a stage where technology is not even yet at the &#x27;trust but verify&#x27; level.</p>
<p>So instead of hallucinating with confidence, when data is unavailable, we prompt the model to return that there was no real-time information accessible to answer the query.</p>
<p align="center"><img width="900" src="/blog/2024-09-21-chatgpt-and-the-future-of-ai-in-finance_4.png"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="function-calling-to-increase-accuracy">Function calling to increase accuracy<a href="#function-calling-to-increase-accuracy" class="hash-link" aria-label="Direct link to Function calling to increase accuracy" title="Direct link to Function calling to increase accuracy">â€‹</a></h3>
<p>One thing we found that significantly reduces hallucinations is enabling our agent, OpenBB Copilot, to have access to all the API backends that users have through OpenBB or those they&#x27;ve added themselves.</p>
<p>Here&#x27;s the sequence of actions that happen:</p>
<ol>
<li>The user asks the OpenBB Copilot a question.</li>
<li>The prompt is converted into embeddings.</li>
<li>We compare that embedding with all the ones that we have on an OpenBB vector store which contains widget signatures - name, description, category, subcategory and source.</li>
<li>We retrieve the widgets with the highest similarity.</li>
<li>The Copilot then decides which widget to use based on the prompt.</li>
<li>Then Copilot also decides what parameters to use when calling that API</li>
</ol>
<br>
<p>This leads to less hallucination because the LLM isn&#x27;t outputting tokens based on a prompt and its internal weights. Instead, it&#x27;s using its internal weights, the prompt, and a function call.</p>
<p>Assuming the function call succeeds - with correct widget retrieval and parameters - the data becomes available for the Copilot to use, which leads to higher accuracy.</p>
<p>Note: This still means that Copilot needs to use the correct widget and the correct parameter, but there&#x27;s a <strong>higher likelihood of success</strong> because if it isn&#x27;t, the API call will fail, prompting the LLM to try again.</p>
<p>Here&#x27;s how it works behind the scenes, the OpenBB Copilot highlights its step-by-step reasoning so users can understand its thought process. Transparency is key.</p>
<p align="center"><img width="900" src="/blog/2024-09-21-chatgpt-and-the-future-of-ai-in-finance_5.png"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="workflows-to-avoid-hallucinations">Workflows to avoid hallucinations<a href="#workflows-to-avoid-hallucinations" class="hash-link" aria-label="Direct link to Workflows to avoid hallucinations" title="Direct link to Workflows to avoid hallucinations">â€‹</a></h3>
<p>In order to reduce the number of hallucinations, there are two things that can be done.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="enable-users-to-quickly-detect-whether-a-hallucination-has-occurred">Enable users to quickly detect whether a hallucination has occurred<a href="#enable-users-to-quickly-detect-whether-a-hallucination-has-occurred" class="hash-link" aria-label="Direct link to Enable users to quickly detect whether a hallucination has occurred" title="Direct link to Enable users to quickly detect whether a hallucination has occurred">â€‹</a></h4>
<p>For instance, if a user utilizes the following prompt on the OpenBB Copilot:</p>
<blockquote>
<p><em>Using the earnings transcript, create a table with columns: financial metric, value, sentence in the earnings where it was extracted from. Double check whether the information you are using is correct.</em></p>
</blockquote>
<br>
<p>They get the &quot;<em>Sentence Extracted From</em>&quot; column, which they can copy and paste into a search field added at the top of the Earnings Transcript widget. This enable users to quickly validate the numbers that have been found.</p>
<p>See example below,</p>
<p align="center"><img width="900" src="/blog/2024-09-21-chatgpt-and-the-future-of-ai-in-finance_6.png"></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="add-deterministic-processes-to-check-for-hallucinations">Add deterministic processes to check for hallucinations<a href="#add-deterministic-processes-to-check-for-hallucinations" class="hash-link" aria-label="Direct link to Add deterministic processes to check for hallucinations" title="Direct link to Add deterministic processes to check for hallucinations">â€‹</a></h4>
<p>For example, let&#x27;s say the user prompt involves a data retrieval task.</p>
<p>We can run a deterministic process to check whether the retrieved values exist or not. Sure this won&#x27;t be 100% accurate because the numbers could be flagged by referring to another thing, BUT it&#x27;s all about improving the overall accuracy of Copilot.</p>
<p>Ultimately, whatever can be done to improve the Copilot&#x27;s accuracy should be done.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="2-agents-are-the-future">2. Agents are the future<a href="#2-agents-are-the-future" class="hash-link" aria-label="Direct link to 2. Agents are the future" title="Direct link to 2. Agents are the future">â€‹</a></h2>
<p>When we think about how humans operate, we recognize that the brain coordinates all the actions of our body and our thought processes. This is similar to how agents work.</p>
<p>If I&#x27;m playing soccer, the muscles I use are different from those I would use if I were boxing. If I&#x27;m programming, the parts of my brain I use differ from those I would use when listening to music.</p>
<p>However, it&#x27;s not as simple as &quot;activity A requires legs&quot;. Most of your body and mind are always involved, but at different times and in different capacities. And what dictates that are external factors.</p>
<p>For instance, if I am playing soccer as a winger and my team is attacking, I will likely be using both legs to run forward and a lot of mental energy to decide where to position myself on the field.</p>
<p>And that will change a lot based on where the ball is. If the ball is on the opposite side, I&#x27;ll likely run less and stay more in the middle to be ready for a counterattack. If the ball is in the middle, I&#x27;ll probably be running at full speed to create space. If the ball is close to me I have to worry more about controlling it and understand what I can do with it next.</p>
<p>The environment affects my plan to carry out an action where I want to have a successful outcome.</p>
<p><strong>This is how agents work.</strong></p>
<p>Agents aren&#x27;t just about a single LLM performing well, but about a full workflow that interacts with multiple language models, function calls, or any other process to carry an action.</p>
<p>At the core, the biggest advantage of an agent over a LLM is that an agent has a full feedback loop. It understands the impact of the LLM output and can use that data in the next step of the process. Whereas a single LLM API call returns its best output but won&#x27;t know how that affected the external environment.</p>
<p>This is why, at OpenBB, we believe in compound AI systems.</p>
<p>And apparently, <a href="https://finance.yahoo.com/news/sequoia-sees-bigger-money-ai-203655254.html?guccounter=1" target="_blank" rel="noopener noreferrer">so does Sequoia</a>.</p>
<p align="center"><img width="300" src="/blog/2024-09-21-chatgpt-and-the-future-of-ai-in-finance_7.png"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="the-strawberry-issue-will-be-solved">The &quot;Strawberry&quot; issue will be solved<a href="#the-strawberry-issue-will-be-solved" class="hash-link" aria-label="Direct link to The &quot;Strawberry&quot; issue will be solved" title="Direct link to The &quot;Strawberry&quot; issue will be solved">â€‹</a></h3>
<p>A panelist commented on stage that LLMs can&#x27;t even count how many R&#x27;s are in the word &quot;Strawberry&quot;.</p>
<p>This <a href="https://x.com/MwangoCapital/status/1828857579860095428" target="_blank" rel="noopener noreferrer">tweet</a> offers a good explanation of why this happens â€” it turns out it&#x27;s due to the tokenizer, and it can be solved. In fact, it&#x27;s solved by simply ensuring that the model takes each letter as a token. See below,</p>
<div class="flex justify-center items-center"><img width="300" src="/blog/2024-09-21-chatgpt-and-the-future-of-ai-in-finance_8.png" style="margin-right:10px"><img width="300" src="/blog/2024-09-21-chatgpt-and-the-future-of-ai-in-finance_9.png"></div>
<br>
<p>This means that the model&#x27;s output can be improved by doing extra work at the input level.</p>
<p>Data cleaning and pre-processing strikes again? ðŸ˜ƒ</p>
<p>Interestingly, a few days ago, <a href="https://openai.com/o1/" target="_blank" rel="noopener noreferrer">OpenAI announced OpenAI o1</a>. Which is basically GPT-4o with Chain-of-Thought (COT). This means that this model is a &quot;wannabe agent&quot;.</p>
<p>It takes in a prompt from the user and decomposes it in natural steps to solve it. Then at each step, it takes the output of the model from the previous step and predicts the next token. It turns out that this improves accuracy substantially.</p>
<p>However, it still doesn&#x27;t have access to external data. And that is why I call it a &quot;wannabe agent&quot;.</p>
<p>I love how Jeremiah put it in this <a href="https://x.com/jlowin/status/1834722014839418962" target="_blank" rel="noopener noreferrer">tweet</a>:</p>
<blockquote>
<p>(...) Agents are also characterized by iterative behavior. But there&#x27;s a key difference: while models like o1 iterate internally to refine their reasoning, agents engage in iterative interactions with the external world. They perceive the environment, take actions, observe the outcomes (or side effects) and adjust accordingly. This recursive process enables agents to handle tasks that require adaptability and responsiveness to real-world changes. (...)</p>
</blockquote>
<br>
<p>So, o1&#x27;s model isn&#x27;t an agent - but it can solve this problem. The reason is that it applies its own data cleaning/pre-processing step on its own, and doesn&#x27;t rely on external factors.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="small-language-models">Small Language Models<a href="#small-language-models" class="hash-link" aria-label="Direct link to Small Language Models" title="Direct link to Small Language Models">â€‹</a></h3>
<p>Once agents work, Small Language Models (SLM) will be much more viable for very specific use cases.</p>
<p>In logical terms, a Large Language Model is a model with weights.</p>
<p>Large means that it has a lot of them. But what tends to happen is LLMs need to be very big because they want these models to be really good at everything. The problem is that if you want the exact same model to be good at discussing soccer, programming, and speaking Portuguese, its weights are updated using these drastically different datasets. Now the premise is that the more weights there are, the less each weight will be affected by data from completely different domains.</p>
<p>What a big LLM like GPT-4o is doing is trying to build a single Jarvis that knows about everything. Whereas we could have an SLM that does something extremely well and just focus on that, e.g. translating from English to Portuguese. The benefit of an SLM is that inference is likely faster, can be hosted on devices, and, in theory, it&#x27;s better on a topic because it&#x27;s been less &quot;contaminated&quot; during training by data that doesn&#x27;t relate to the task at hand.</p>
<p>Imagine that a firm decides to use an SLM trained to retrieve data from SEC filings quickly and at scale. Or, we could train our own SLM to understand user intent and interact directly with the OpenBB Terminal interface.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="large-language-models-as-orchestrator">Large Language Models as orchestrator<a href="#large-language-models-as-orchestrator" class="hash-link" aria-label="Direct link to Large Language Models as orchestrator" title="Direct link to Large Language Models as orchestrator">â€‹</a></h3>
<p>In my opinion, the best LLM in each category will win. And the second and third won&#x27;t matter. It&#x27;s a winner-takes-all kind of market. Unless in specific verticals such as inference time or open weights (e.g. for data security; more on this later).</p>
<p>The best example of this is OpenAI vs Anthropic.</p>
<p>I had been using OpenAI&#x27;s GPT-4 for coding for several months. After trying Anthropic&#x27;s Sonnet 3.5 for coding, I never went back to OpenAI.</p>
<p align="center"><img width="400" src="/blog/2024-09-21-chatgpt-and-the-future-of-ai-in-finance_10.png"></p>
<p>The market share for the best LLM will be gigantic. That&#x27;s why <a href="https://www.bloomberg.com/news/articles/2024-09-11/openai-fundraising-set-to-vault-startup&#x27;s-value-to-150-billion" target="_blank" rel="noopener noreferrer">OpenAI is looking to raise at a $150 billion valuation</a>. While the valuation reflects the market size, the amount that will be raised represents the capital needed to reach that valuation. This is why only a few players will be able to compete at that level.</p>
<p>In an &quot;agentic future&quot;, I believe the best LLM will serve as the core &quot;brain&quot; - the main LLM that routes all prompts and decides what happens next.</p>
<p>And who wouldn&#x27;t want the smartest model controlling the actions with a list of models, functions and data at its disposal?</p>
<p>I know I would.</p>
<p>That&#x27;s also why, when discussing OpenBB Copilot, we don&#x27;t rely on a single foundational model. Instead, we use the models that are best suited for each specific task.</p>
<p>For instance, OpenAI o1 can be the brains, but when a user uses @web it triggers the Perplexity model, and when they upload an image, we have Anthropic&#x27;s Haiku. Or maybe if they want to do intraday trading, we use Llama 3.1 through Groq for fast inference.</p>
<p>You get the idea.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="3-when-does-it-make-sense-to-fine-tune">3. When does it make sense to fine-tune<a href="#3-when-does-it-make-sense-to-fine-tune" class="hash-link" aria-label="Direct link to 3. When does it make sense to fine-tune" title="Direct link to 3. When does it make sense to fine-tune">â€‹</a></h2>
<p>A good comment was made on the panel: &quot;<em>it&#x27;s expensive to spend time fine-tuning a new model, just for that entire work to be &#x27;eradicated&#x27; by a new model that has a higher performance in that specific domain than the model has been fine-tuned</em>&quot;.</p>
<p>In my opinion, this happens because the timing isn&#x27;t right yet. We are still unlocking remarkable achievements through each new model release. Although there is a massive bump in terms of capability between these releases, I wouldn&#x27;t recommend that a firm fine-tune its own models at this stage.</p>
<p>However, at some point, whether due to a lack of data to train or architecture needing to be reinvented, improvements in LLM performance won&#x27;t be substantial - they may not even be noticeable. This is when the fine-tuning technique becomes relevant because at this stage you are trying to repurpose everything the model has towards a specific vertical / use-case - and at that vertical/use-case that model will be better than the following one.</p>
<p>Then after some new models come out, you may consider reapplying fine-tuning to that model, but this would likely be years later, not weeks or months. So, the ROI can be quite high. Particularly when you are trying to win in your specific market.</p>
<p>This is how I see it working in my head:</p>
<p align="center"><img width="900" src="/blog/2024-09-21-chatgpt-and-the-future-of-ai-in-finance_11.png"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="4-compliance-and-data-security">4. Compliance and Data security<a href="#4-compliance-and-data-security" class="hash-link" aria-label="Direct link to 4. Compliance and Data security" title="Direct link to 4. Compliance and Data security">â€‹</a></h2>
<p>Another question I received was about compliance and data security.</p>
<p>Recently, during a discussion with one of the largest hedge funds in the world, we were asked about the entire workflow of the data when our AI Copilot has access to it.</p>
<p>Their main concern was ensuring that no data was being shared with third-party vendors like OpenAI. For such firms, their data is their alpha, and keeping it within their network is paramount.</p>
<p>Crypto enthusiasts often say, &quot;Not your keys, not your coins&quot; to emphasize the importance of storing assets in a cold wallet rather than leaving them on an exchange that might implode (looking at you, FTX). The same principle applies here: &quot;Not your weights, not your data&quot;.</p>
<p>When you send information to a large foundation model provider like OpenAI, your data enters their ecosystem, and you have to trust they&#x27;ll honor the terms of your contract.</p>
<p>A more secure approach is to host an open-source model locally within your firm, ensuring that sensitive data remains entirely within your infrastructure and network.</p>
<p>Although open-source models aren&#x27;t yet as powerful as closed-source ones, they are catching up quickly. If you think that GPT-4o can already do a lot for you, think about how at some point there will be an open-source model that is GPT-4o equivalent. Sure, at that time closed-source models will be better, but the question is: How much better?</p>
<p>Or better, the question is: <strong>&quot;How much are you willing to sacrifice in terms of data security for performance?&quot;</strong>.</p>
<p>At OpenBB, we take this very seriously and have taken measures to allow enterprise customers to fully control their data.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="bring-your-own-copilot">Bring your own copilot<a href="#bring-your-own-copilot" class="hash-link" aria-label="Direct link to Bring your own copilot" title="Direct link to Bring your own copilot">â€‹</a></h3>
<p>Enable firms to bring their own LLMs to access data within OpenBB. This means that we provide an interface for research, but also allow them to integrate their internal LLMs and interact directly with it from OpenBB.</p>
<div class="flex justify-center items-center"><img width="350" src="/blog/2024-09-21-chatgpt-and-the-future-of-ai-in-finance_12.png" style="margin-right:10px"><img width="350" src="/blog/2024-09-21-chatgpt-and-the-future-of-ai-in-finance_13.png"></div>
<br>
<p>We believe in this idea so much, that we have open-source the architecture for firms to bring their own Copilot to OpenBB. More information is available <a href="https://github.com/OpenBB-finance/copilot-for-terminal-pro/" target="_blank" rel="noopener noreferrer">here</a>.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="turn-off-ai-workflows">Turn off AI workflows<a href="#turn-off-ai-workflows" class="hash-link" aria-label="Direct link to Turn off AI workflows" title="Direct link to Turn off AI workflows">â€‹</a></h3>
<p>We have incorporated workflows that make users&#x27; lives MUCH better. But they come at a cost: sharing data with an LLM provider.</p>
<p>These are the features:</p>
<ul>
<li><strong>Widget title/description suggestion from Copilot</strong>: This sends the content of the table or note output by Copilot to an LLM provider to receive suggestions of a title and description.</li>
</ul>
<p align="center"><img width="600" src="/blog/2024-09-21-chatgpt-and-the-future-of-ai-in-finance_14.png"></p>
<ul>
<li><strong>Widget title/description suggestion upon upload</strong>: It sends the content of the file that has been uploaded to an LLM provider to receive suggestions of title and description.</li>
</ul>
<p align="center"><img width="600" src="/blog/2024-09-21-chatgpt-and-the-future-of-ai-in-finance_15.png"></p>
<ul>
<li><strong>Copilot chat title generation</strong>: Upon the first user prompt, the content is sent to an LLM provider to update the chat title, reflecting the nature of the conversation.</li>
</ul>
<p align="center"><img width="600" src="/blog/2024-09-21-chatgpt-and-the-future-of-ai-in-finance_16.png"></p>
<ul>
<li><strong>Dashboard name generation</strong>: When renaming the dashboard, we send the title and descriptions of all widgets on that dashboard to an LLM provider, to ensure that the suggested name is relevant.</li>
</ul>
<p align="center"><img width="600" src="/blog/2024-09-21-chatgpt-and-the-future-of-ai-in-finance_17.png"></p>
<p>To allow firms to keep their data within their network, one of our enterprise features is the option to disable these AI workflows.</p>
<p align="center"><img width="900" src="/blog/2024-09-21-chatgpt-and-the-future-of-ai-in-finance_18.png"></p>
<p>In the future, we could direct these AI workflows to use an LLM that our customers are running locally.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="so-in-a-nutshell-what-can-you-expect-from-openbb">So, in a nutshell, what can you expect from OpenBB?<a href="#so-in-a-nutshell-what-can-you-expect-from-openbb" class="hash-link" aria-label="Direct link to So, in a nutshell, what can you expect from OpenBB?" title="Direct link to So, in a nutshell, what can you expect from OpenBB?">â€‹</a></h2>
<p>We are building an AI-powered research workspace.</p>
<p>At the core it is an AI compound system, where users can bring their own data (structured, unstructured, API, custom backend, database, data warehouse, etc..) and have our (or their own copilot) access all this data seamlessly - in an interface that is customizable, flexible and enables teams to work together.</p>
<p>If you want to learn more, e-mail me directly at <a href="mailto:didier.lopes@openbb.finance" target="_blank" rel="noopener noreferrer">didier.lopes@openbb.finance</a></p></div><footer class="docusaurus-mt-lg"><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/blog/tags/ai">ai</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/blog/tags/fintech">fintech</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/blog/tags/llm">llm</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/blog/tags/agents">agents</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/blog/tags/hallucinations">hallucinations</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/blog/tags/fine-tuning">fine-tuning</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/blog/tags/data-security">data-security</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/blog/tags/openbb">openbb</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/blog/tags/thought-leadership">thought-leadership</a></li></ul></div></div><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><a href="https://github.com/DidierRLopes/my-website/tree/main/blog/2024-09-21-chatgpt-and-the-future-of-ai-in-finance.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article></div><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/blog/openbb-mobile-app-coming-soon"><div class="pagination-nav__sublabel">Newer post</div><div class="pagination-nav__label">OpenBB Mobile App - Coming soon</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/blog/why-i-love-boxing"><div class="pagination-nav__sublabel">Older post</div><div class="pagination-nav__label">Why I love boxing</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#1-hallucinations" class="table-of-contents__link toc-highlight">1. Hallucinations</a><ul><li><a href="#confident-hallucinations" class="table-of-contents__link toc-highlight">Confident hallucinations</a></li><li><a href="#how-to-avoid-hallucinations" class="table-of-contents__link toc-highlight">How to avoid hallucinations</a></li><li><a href="#function-calling-to-increase-accuracy" class="table-of-contents__link toc-highlight">Function calling to increase accuracy</a></li><li><a href="#workflows-to-avoid-hallucinations" class="table-of-contents__link toc-highlight">Workflows to avoid hallucinations</a></li></ul></li><li><a href="#2-agents-are-the-future" class="table-of-contents__link toc-highlight">2. Agents are the future</a><ul><li><a href="#the-strawberry-issue-will-be-solved" class="table-of-contents__link toc-highlight">The &quot;Strawberry&quot; issue will be solved</a></li><li><a href="#small-language-models" class="table-of-contents__link toc-highlight">Small Language Models</a></li><li><a href="#large-language-models-as-orchestrator" class="table-of-contents__link toc-highlight">Large Language Models as orchestrator</a></li></ul></li><li><a href="#3-when-does-it-make-sense-to-fine-tune" class="table-of-contents__link toc-highlight">3. When does it make sense to fine-tune</a></li><li><a href="#4-compliance-and-data-security" class="table-of-contents__link toc-highlight">4. Compliance and Data security</a><ul><li><a href="#bring-your-own-copilot" class="table-of-contents__link toc-highlight">Bring your own copilot</a></li><li><a href="#turn-off-ai-workflows" class="table-of-contents__link toc-highlight">Turn off AI workflows</a></li></ul></li><li><a href="#so-in-a-nutshell-what-can-you-expect-from-openbb" class="table-of-contents__link toc-highlight">So, in a nutshell, what can you expect from OpenBB?</a></li></ul></div></div></div></div></div><div class="mt-4"><footer class="bg-[#f8f9fa] dark:bg-[#000] text-[var(--ifm-footer-color)] pb-8"><div class="container container-fluid"><div class="row flex justify-center items-center"><div class="col text-center"><h4 class="mb-4">Let&#x27;s stay in touch.</h4><div class="flex justify-center space-x-4"><a href="https://twitter.com/didier_lopes" target="_blank" rel="noreferrer" class="text-black dark:text-white hover:text-ds-blue-primary dark:hover:text-ds-blue-accent"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" aria-label="Twitter"><title>Twitter</title><path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"></path></svg></a><a href="https://www.linkedin.com/in/didier-lopes/" target="_blank" rel="noreferrer" class="text-black dark:text-white hover:text-ds-blue-primary dark:hover:text-ds-blue-accent"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" aria-label="LinkedIn"><title>LinkedIn</title><path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z"></path></svg></a><a href="https://github.com/DidierRLopes" target="_blank" rel="noreferrer" class="text-[#333] dark:text-[#f5f5f5] hover:text-ds-blue-primary dark:hover:text-ds-blue-accent"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" aria-label="GitHub"><title>GitHub</title><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg></a><a href="https://cal.com/didierlopes/15min" target="_blank" rel="noreferrer" class="text-black dark:text-white hover:text-ds-blue-primary dark:hover:text-ds-blue-accent" aria-label="Book a meeting"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" aria-label="Calendar"><title>Cal.com</title><path d="M19 4h-1V2h-2v2H8V2H6v2H5c-1.11 0-1.99.9-1.99 2L3 20c0 1.1.89 2 2 2h14c1.1 0 2-.9 2-2V6c0-1.1-.9-2-2-2zm0 16H5V10h14v10zM9 14H7v-2h2v2zm4 0h-2v-2h2v2zm4 0h-2v-2h2v2zm-8 4H7v-2h2v2zm4 0h-2v-2h2v2zm4 0h-2v-2h2v2z"></path></svg></a></div></div></div></div></footer></div></div>
</body>
</html>